---
title: "Generalized Linear Models"
subtitle: '"The trouble with normal is that it always gets worse"'
author: "Samuel Robinson, Ph.D."
date: "Sept 29, 2023"
output: 
  beamer_presentation:
    incremental: true
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
    fig_caption: false
urlcolor: blue
df_print: kable
classoption: aspectratio=169    
header-includes: 
  - \usepackage{tikz}
  - \usepackage{tikzit}
  - \input{styles.tikzstyles}
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
  - \let\oldShaded\Shaded %Change fontsize of R code chunks
  - \let\endoldShaded\endShaded
  - \renewenvironment{Shaded}{\scriptsize\oldShaded}{\endoldShaded}
  - \let\oldverbatim\verbatim %Change fontsize of code chunk output
  - \let\endoldverbatim\endverbatim
  - \renewenvironment{verbatim}{\tiny\oldverbatim}{\endoldverbatim}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, warning=TRUE)

library(MASS) #glm.nb
library(tidyverse)
theme_set(theme_classic())
library(ggeffects)
library(ggpubr)
library(knitr)
library(kableExtra)
library(latex2exp)
library(rmutil) #Beta-binomial

set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Generate data that violate lm assumptions:
n <- 100
x <- runif(n,-10,10)
yhat <- 1 - 0.2*x #Expected value
y0 <- yhat + rnorm(n,0,2) #OK
y1 <- rpois(n,exp(yhat))  #Poisson process
y2 <- rbinom(n,1,invLogit(yhat))  #Binomial process

d1 <- data.frame(x,yhat,y0,y1,y2) #Dataframe


```

# Part 1: The exponential family

## Outline 

::: columns

:::: column

- Meet (some of) the exponential family!
  - Normal
  - Binomial
  - Poisson
  - Beta-Binomial
  - Negative Binomial
- "Play time"

:::: 

:::: column

![](pillows.jpg)

Christmas gifts for the nerds in your life

::::

:::

## Problem: not everything is normal

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2} 
ggplot(d1,aes(x,y2))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

- Some types of data can never be transformed to make the residuals normal
- Solution: __use the distribution that generates the data!__

:::: 

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3.5} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y2~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## But how do I know which distribution to use?

![](distributions.png){width=100%}   
[_And if thou gaze long into an abyss, the abyss will also gaze into thee_ - F. Nietzsche](http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf)

## Let's take a look at some _common_ ones!

```{r, fig.height=2.75, fig.width=5} 
n <- 1000
dists <- c('Normal','Binomial','Poisson','Bernoulli')
data.frame(dist=factor(rep(dists,each=n),levels=dists),
           x=c(rnorm(n,0,1),rbinom(n,10,0.25),rpois(n,1),rbinom(n,1,0.6))) %>% 
  ggplot(aes(x=x))+geom_histogram(bins=30)+facet_wrap(~dist,scales='free')+labs(x=NULL,y=NULL)
```

Time to meet the Exponential family!

## The Normal Distribution (aka _Gaussian_)

::: columns

:::: column

- Imagine many random + and - numbers added together
- If you do this _many_ times:
  - Most cancel out (somewhere around 0)
  - Few are far away from 0 (tails of distribution)
- Common in nature, because of many small + and - factors adding together
  - e.g. Height is driven by many sets of genes

::::

:::: column

[A Galton Board in action](https://d.stockcharts.com/img/articles/2019/04/1554241718392985801900.gif): ![](galtonBoard.jpg)

::::

:::

## The Normal Distribution - scary math!

::: columns

:::: column

- 2 parameters: mean ($\mu$) and standard deviation ($\sigma$)

\begin{equation*}
p(\textcolor{blue}{x}|\textcolor{darkturquoise}{\mu},\textcolor{red}{\sigma}) = \frac{1}{\textcolor{red}{\sigma}\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{\textcolor{blue}{x}-\textcolor{darkturquoise}{\mu}}{\textcolor{red}{\sigma}})^2}
\end{equation*}

- Probability distribution function (PDF) for the Normal distribution
- Tells you about the probability of getting some number _given_ $\mu$ and $\sigma$

::::

:::: column

\pause

Example: what is the probability of getting a 4, if the mean is 5 and SD is 1?

\begin{equation*}
\begin{split}
p(\textcolor{blue}{4}|\textcolor{darkturquoise}{5},\textcolor{red}{1}) = & \frac{1}{\textcolor{red}{1}\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{\textcolor{blue}{4}-\textcolor{darkturquoise}{5}}{1})^2}\\
= \sim & 0.24
\end{split}
\end{equation*}

In R, this is easy:

\pause

```{r, echo=TRUE}
#d stands for "density"
dnorm(x=4,mean=5,sd=1)
```

::::

:::

## The Normal Distribution

::: columns

:::: column

```{r, fig.height=5, fig.width=5}
data.frame(x=c(0:10)) %>% 
  ggplot(aes(x=x))+stat_function(fun=dnorm,n=100,args=list(mean=5,sd=1))+
  geom_vline(xintercept=4,linetype='dashed',color='blue')+
  annotate('text',x=3,y=dnorm(4,5,1),label=round(dnorm(4,5,1),3),color='blue')+
  annotate('text',x=7.5,y=dnorm(4,5,1)*1.1,label='mu == 5',colour='darkturquoise',size=6,parse=TRUE)+
  annotate('text',x=7.5,y=dnorm(4,5,1)*1,label='sigma==1',colour='red',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\mu,\\sigma)$'))+
  ylim(0,0.4)
```

::::

:::: column

```{r, fig.height=5, fig.width=5}
data.frame(x=c(0:10)) %>% 
  ggplot(aes(x=x))+stat_function(fun=dnorm,n=100,args=list(mean=5,sd=3))+
  geom_vline(xintercept=4,linetype='dashed',color='blue')+
  annotate('text',x=3,y=dnorm(4,5,3)+0.01,label=round(dnorm(4,5,3),3),color='blue')+
  annotate('text',x=7.5,y=dnorm(4,5,3)*1.2,label='mu == 5',colour='darkturquoise',size=6,parse=TRUE)+
  annotate('text',x=7.5,y=dnorm(4,5,3)*1,label='sigma==3',colour='red',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\mu,\\sigma)$'))+
  ylim(0,0.4)
```

::::

:::

- Probability of x changes with $\mu$ and $\sigma$
- Left: $\sigma = 1$, Right: $\sigma = 3$

## The Binomial Distribution

::: columns

:::: column

- Imagine you have 10 coins, and you flip them all
- If you do this _many_ times:
  - Most will be about 5 heads/tails
  - Few will be 1 head, 9 tails (or reverse)
- Common in nature where outcomes are binary
  - e.g. 10 seeds from a plant, how many will germinate?
- If N = 1, this is called a _Bernoulli trial_

::::

:::: column

```{r, fig.height=6, fig.width=5}
data.frame(x=0:10) %>% mutate(d=dbinom(x,10,0.5)) %>% 
  ggplot(aes(x=x,y=d))+geom_col()+
  labs(x='Number of heads per 10 flips',y='Frequency')+
  scale_x_continuous(breaks=-1:10)
```

::::

:::


## The Binomial Distribution - scary math!

::: columns

:::: column

- 1 parameter: probability of success ($\phi$), plus...
- Number of "coin flips" ($N$)

\small

\begin{equation*}
p(\textcolor{blue}{x}|\textcolor{darkturquoise}{\phi},N) = \binom{N}{\textcolor{blue}{x}} \textcolor{darkturquoise}{\phi}^{\textcolor{blue}{x}} (1-\textcolor{darkturquoise}{\phi})^{N-\textcolor{blue}{x}} 
\end{equation*}

\normalsize

- Probability mass function (PMF); density = continuous
- Tells you about the probability of getting $\textcolor{blue}{x}$ "successes" _given_ $\phi$ and $N$

::::

:::: column

\pause

Example: what is the probability of getting 4 successes, if $\textcolor{darkturquoise}{\phi}$ is 0.25 and N is 15?

\small

\begin{equation*}
\begin{split}
p(\textcolor{blue}{4}|\textcolor{darkturquoise}{0.25},15) = & \binom{15}{\textcolor{blue}{4}} \textcolor{darkturquoise}{0.25}^{\textcolor{blue}{4}} (1-\textcolor{darkturquoise}{0.25})^{15-\textcolor{blue}{4}}  \\
= \sim & 0.23
\end{split}
\end{equation*}

\pause

In R, this is easy:

```{r, echo=TRUE}
dbinom(x=4,size=15,prob=0.25)
```

::::

:::

## The Binomial Distribution

::: columns

:::: column

```{r, fig.height=5, fig.width=5}
nSuccess <- 4
n <- 15
phi <- 0.25
data.frame(x=c(0:15)) %>% mutate(d=dbinom(x,n,phi)) %>% 
  ggplot(aes(x=factor(x),y=d))+
  geom_point()+
  geom_vline(xintercept=nSuccess+1,linetype='dashed',color='blue')+
  annotate('text',x=6,y=dbinom(nSuccess,n,phi),label=round(dbinom(nSuccess,n,phi),3),color='blue')+
  annotate('text',x=9.5,y=dbinom(nSuccess,n,phi)*1.1,label=paste('phi == ',phi),colour='darkturquoise',size=6,parse=TRUE)+
  annotate('text',x=9.5,y=dbinom(nSuccess,n,phi)*1,label=paste('N == ',n),colour='black',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\phi,N)$'),x='Number of successes')+
  ylim(0,0.25)
```

::::

:::: column

```{r, fig.height=5, fig.width=5}
nSuccess <- 4
n <- 15
phi <- 0.75
data.frame(x=c(0:15)) %>% mutate(d=dbinom(x,n,phi)) %>% 
  ggplot(aes(x=factor(x),y=d))+
  geom_point()+
  geom_vline(xintercept=nSuccess+1,linetype='dashed',color='blue')+
  annotate('text',x=4,y=dbinom(nSuccess,n,phi)+0.01,label=round(dbinom(nSuccess,n,phi),4),color='blue')+
  annotate('text',x=9.5,y=0.22,label=paste('phi == ',phi),colour='darkturquoise',size=6,parse=TRUE)+
  annotate('text',x=9.5,y=0.2,label=paste('N == ',n),colour='black',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\phi,N)$'),x='Number of successes')+
  ylim(0,0.25)
```

::::

:::

- Probability of x "successes" changes with $\phi$ and $N$

## The Poisson Distribution

::: columns

:::: column

- Imagine a rare event (e.g. getting a non-junk mail letter)
- If you record the number of events every day:
  - Most days, you'll get 0 or maybe 1 letter
  - On some rare days, you'll get 3 or 4 letters
- Common in nature where rare events are measured over time/space:
  - e.g. Number of bugs caught in a net (per sweep)
- Equivalent to Binomial distribution, where $N$ is unknown

::::

:::: column

```{r, fig.height=6, fig.width=5}
data.frame(x=0:8) %>% mutate(d=dpois(x,1)) %>% 
  ggplot(aes(x=x,y=d))+geom_col()+
  labs(x='Letters per day',y='Density')
```


::::

:::

## The Poisson Distribution - scary math!

::: columns

:::: column

- 1 parameter: rate parameter ($\lambda$)

\begin{equation*}
p(\textcolor{blue}{x}|\textcolor{darkturquoise}{\lambda}) = \frac{\textcolor{darkturquoise}{\lambda} ^{\textcolor{blue}{x}} e ^{-\textcolor{darkturquoise}{\lambda}}}{\textcolor{blue}{x}!}
\end{equation*}

- Probability mass function (PMF)
- Tells you about the probability of getting $\textcolor{blue}{x}$ counts _given_ $\lambda$ 

::::

:::: column

\pause

Example: what is the probability of getting 2 counts, if $\textcolor{darkturquoise}{\lambda}$ is 1?

\small

\begin{equation*}
\begin{split}
p(\textcolor{blue}{2}|\textcolor{darkturquoise}{1}) = & \frac{\textcolor{darkturquoise}{1} ^{\textcolor{blue}{2}} e ^{-\textcolor{darkturquoise}{1}}}{\textcolor{blue}{2}!} \\
= &\sim 0.18
\end{split}
\end{equation*}

\pause

In R, this is easy:

```{r, echo=TRUE}
dpois(x=2,lambda=1)
```

::::

:::

## The Poisson Distribution

::: columns

:::: column

```{r, fig.height=5, fig.width=5}
nCounts <- 2
lambda <- 1
data.frame(x=c(0:8)) %>% mutate(d=dpois(x,lambda)) %>% 
  ggplot(aes(x=factor(x),y=d))+
  geom_point()+
  geom_vline(xintercept=nCounts+1,linetype='dashed',color='blue')+
  annotate('text',x=nCounts+1.5,y=dpois(nCounts,lambda),label=round(dpois(nCounts,lambda),3),color='blue')+
  annotate('text',x=nCounts+2,y=dpois(nCounts,lambda)*1.8,label=paste('lambda == ',lambda),colour='darkturquoise',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\lambda)$'),x='Counts observed')+
  ylim(0,0.4)
```

::::

:::: column

```{r, fig.height=5, fig.width=5}
nCounts <- 2
lambda <- 3
data.frame(x=c(0:8)) %>% mutate(d=dpois(x,lambda)) %>% 
  ggplot(aes(x=factor(x),y=d))+
  geom_point()+
  geom_vline(xintercept=nCounts+1,linetype='dashed',color='blue')+
  annotate('text',x=nCounts+1.5,y=dpois(nCounts,lambda),label=round(dpois(nCounts,lambda),3),color='blue')+
  annotate('text',x=nCounts+2,y=dpois(nCounts,lambda)*1.5,label=paste('lambda == ',lambda),colour='darkturquoise',size=6,parse=TRUE)+
  labs(y=TeX('$p(x|\\lambda)$'),x='Counts observed')+
  ylim(0,0.4)
```

::::

:::

- Probability of x counts changes with $\lambda$



## More complications:

- The Normal distribution has a parameter for the mean and SD, but...
- What about the Binomial and Poisson distributions?
  - Binomial: mean $= Np$, SD $= \sqrt{Np(1-p)}$
  - Poisson: mean $= \lambda$, SD $= \sqrt{\lambda}$
- What if our data have additional variance?
  - _Beta Binomial_ and _Negative Binomial_ distributions
  
## The Beta Binomial Distibution

::: columns

:::: column

- Many "coin-flip" processes have longer tails than standard Binomial
  - e.g. numbers of males/females in families
- Beta-binomial adds additional dispersion to coin flip process
- 2 parameters: $\phi$ and $s$ (if $s$ is large, similar to Binomial) 
  - Also requires: $N$
  
\pause
  
```{r, eval=FALSE, echo=TRUE}
#Extra distributions
library(rmutil) 
dbetabinom(x,m=phi,size=N,s=5)
```

::::

:::: column

```{r, fig.height=6, fig.width=5}
n <- 10
phi <- 0.5
data.frame(x=0:10) %>% mutate(bin=dbinom(x,size=n,prob=phi),betabin=dbetabinom(x,size=n,m=phi,s=5)) %>% 
  pivot_longer(cols=bin:betabin,names_to='dist',values_to='d') %>% 
  mutate(dist=factor(dist,levels=c('bin','betabin'),labels=c('Binomial','Beta Binomial (s=5)'))) %>% 
  ggplot(aes(x=x,y=d))+geom_col()+facet_wrap(~dist,ncol=1) +
  labs(x='Number of heads per 10 flips',y='Density')+scale_x_continuous(breaks=-1:10)
```

::::

:::

## The Negative Binomial Distribution

::: columns

:::: column

- Unfortunately, _almost nothing_ in ecology actually follows a Poisson distribution 
- Negative Binomial is similar to a Poisson, but can have longer tails
- Also called: _Polya_ distibution (`nbinom2` in many GLM commands)
- Parameters: $\mu$ and $\theta$ (if $\theta$ is large, close to Poisson)

\pause
  
```{r,eval=FALSE,echo=TRUE}
#size = theta parameter
dnbinom(x,mu,size=1) 
```

::::

:::: column

```{r, fig.height=6, fig.width=5}
lambda <- 1
data.frame(x=0:10) %>% mutate(pois=dpois(x,lambda=lambda),nb=dnbinom(x,mu=lambda,size=1)) %>% 
  pivot_longer(cols=pois:nb,names_to='dist',values_to='d') %>% 
  mutate(dist=factor(dist,levels=c('pois','nb'),labels=c('Poisson','Negative Binomial (theta = 1)'))) %>% 
  ggplot(aes(x=x,y=d))+geom_col()+facet_wrap(~dist,ncol=1) +
  labs(x='Number of counts',y='Density')
```

::::

:::

## Summary of Common "Starter" Distributions

- Continuous data, spanning - or + numbers:
  - Normal (transformed or regular)
- Count data
  - Poisson, Negative Binomial
- Count data of successes _and_ failures
  - Binomial, Beta Binomial

\pause

These are by _no means_ the only useful distributions, but are fairly common

## First challenge (Part 1)

Let's say that you've collected data at 2 different sites. Which distributions would you start with for the following data?

- Insects caught in a trap (per day)
- Weight of seeds from a plant
- Occupied/unoccupied nest sites
- Chemical concentrations
- Size of trees (DBH or height)
- Number of male and female bats

## Second challenge (Part 2)

Now that you've figured out which distribution, try simulating some data from each "site", and plot it!

- Insects caught in a trap (per day): _Poisson or NB_
  - `rpois(n,lambda)` or `rnbinom(n,mu,size)`
- Weight of seeds: _Normal_
  - `rnorm(n,mean,sd)`
- Occupied/unoccupied nest sites: _Binomial_
  - `rbinom(n, 1, prob)` aka. _Bernoulli_ distribution
- Chemical concentrations in a pond: _Normal_
  - `rnorm(n,mean,sd)`
- Size of trees (DBH or height):_log-Normal_
  - `exp(rnorm(n,mean,sd))`
- Number of male and female bats: _Binomial or Beta Binomial_
  - `rbinom(n, size, prob)` or `rbetabinom(n,size,m,s)`

# Part 2: Maximum likelihood and GLMs

```{r setup2, include=FALSE}
set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Generate data that violate lm assumptions:
n <- 100
x1 <- runif(n,-10,10) #Predictors
x2 <- runif(n,-5,5)

yhat <- 1 - 0.2*x1 + 0.3*x2 #Expected value
y0 <- yhat + rnorm(n,0,2) #Normal process
y1 <- rpois(n,exp(yhat))  #Poisson process
y2 <- rbinom(n,1,invLogit(yhat))  #Binomial (bernoulli) process

d1 <- data.frame(x1,x2,yhat,y0,y1,y2) #Dataframe

```

## Outline

- Maximum likelihood
  - A way to think about data
  - Likelihood vs Probability
- Generalized linear models
  - Link functions
  - Predictors -> Linear model

## How is our data made?

Making data can be thought of as a _factory_

- Input: __parameters__ (things that guide the process)
- Process: __probability function__
- Output: __data__ (things made by the process)

![](genProcess.png)

## Examples

![](normProc.png)

![](bernProc.png)



<!-- ## Likelihood vs Probability -->

<!-- These concepts are both related to the chance of seeing something occur -->

<!-- - Probability: "I know that $\mu = 0$ and $\sigma = 1$. What are my chances of getting a number between -1 and +1?" -->
<!--   - $p(\text{data}|\mu=0,\sigma=1)$  -->
<!-- - Likelihood: "I got a bunch of numbers. What is the chance that $\mu = 0$ and $\sigma = 1$ made them?" -->
<!--   - $L(\mu=0,\sigma=1|\text{data})$  -->

<!-- We will mostly deal with _likelihoods_, because we never actually know what $\mu$ and $\sigma$ are! -->

## Likelihood vs Probability

![](bernProb.png)

![](bernLik.png)


## Likelihood vs Probability (cont.)

Probability and likelihood both use the same PDF

- "I know that $\phi$ = 0.3. What is the chance of getting 2 heads and a tail?"

```{r, echo=TRUE}
dbinom(1,1,0.3)*dbinom(1,1,0.3)*dbinom(0,1,0.3)
```

- "I got 2 heads and a tail. What is the likelihood that $\phi$ = 0.3?"

```{r, echo=TRUE}
dbinom(1,1,0.3)*dbinom(1,1,0.3)*dbinom(0,1,0.3)
```

## Likelihood vs Probability (cont.)

Let's see how _likelihood_ changes with different values of $\phi$:

```{r, echo=TRUE}
#phi = 0.3
dbinom(1,1,0.3)*dbinom(1,1,0.3)*dbinom(0,1,0.3)
```

```{r, echo=TRUE}
#phi = 0.7
dbinom(1,1,0.7)*dbinom(1,1,0.7)*dbinom(0,1,0.7)
```

Likelihood of $\phi$ = 0.7 is higher, i.e. $\phi$ = 0.7 matches our data _better_

## Likelihood

```{r, echo=FALSE, eval = TRUE, fig.width= 4, fig.height = 2.5}
llfun <- function(phi) dbinom(1,1,phi)*dbinom(1,1,phi)*dbinom(0,1,phi)

ggplot() + geom_function(fun=llfun) + xlim(0,1) + labs(x=expression(phi),y=expression(paste('Likelihood(H,H,T|',phi,')'))) + geom_vline(xintercept=2/3,linetype='dashed')

```

The best match (maximum likelihood value) is at $\phi$ = 0.666 (2 heads out of 3 flips)

## Generalized Linear Models

`glm()` will fit a model like this, and find the ML solution

```{r, echo=TRUE, size='tiny'}
dat <- data.frame(flips=c(1,1,0)) #Data (2 heads, 1 tail)
mod1 <- glm(flips~1,data=dat,family='binomial') #Note family specification
summary(mod1)
```

Wait... our estimate should be 0.666 (2/3), not `r round(coef(mod1),3)`!

## Link functions

- Some parameters of PDFs have _limits_
  - Normal: $-\infty < \mu < \infty$, $0 < \sigma$
  - Binomial: $0 < \phi < 1$
  - Poisson: $0 < \lambda$
- GLMs use _link functions_ to map values onto the appropriate parameter range
  - Normal: Identity (i.e. $\times 1$)
  - Binomial: Logit
  - Poisson/NB: Log
- $logit(0.693) = 0.666$, so the GLM actually got it right!

## What do these functions look like?

::: columns

:::: column

```{r, echo = FALSE, eval = TRUE, fig.height=4, fig.width=4}
ggplot() + geom_function(fun=logit,n=1000) + xlim(0,1) + labs(y='logit(x)',x='x')+
  annotate('text',x=0.75,y=-3,label='paste(f(x) == ln, bgroup("(", frac("x","1-x"),")"))',parse=TRUE)+
  annotate('text',x=0.75,y=-5,label='f(x)^-1 == frac(e^x,e^x+1)',parse=TRUE)

# invLogit <- function(x) exp(x)/(1+exp(x))

```

::::

:::: column

```{r, echo = FALSE, eval = TRUE, fig.height=4, fig.width=4}
ggplot() + geom_function(fun=log,n=1000) + xlim(0,10) + labs(y='log(x)',x='x')+
  annotate('text',x=6,y=-2,label='f(x) == ln(x)',parse=TRUE)+
  annotate('text',x=6,y=-3,label='f(x)^-1 == e^x',parse=TRUE)
```

::::

:::

- These functions map parameter values from the appropriate range (0-1 or 0-$\infty$) onto $-\infty$ to $+\infty$

## Why do we bother with these link function?

::: columns

:::: column

- Likelihood functions are not symmetrical on the regular scale
- On the link-scale, they are closer to a normal distribution
- Makes it easier for R to find the ML estimate (and confidence intervals)

::::

:::: column

```{r, echo=FALSE, eval = TRUE, fig.width= 3, fig.height = 2}
ggplot() + geom_function(fun=llfun) + xlim(0,1) + labs(x=expression(phi),y=expression(paste('Likelihood(H,H,T|',phi,')'))) + geom_vline(xintercept=2/3,linetype='dashed')

```

```{r, echo=FALSE, eval = TRUE, fig.width= 3, fig.height =2}
#Uses logit-phi (-Inf to +Inf) rather than phi (0 to 1)
llfun2 <- function(lphi){
  phi <- invLogit(lphi)
  ll <- dbinom(1,1,phi)*dbinom(1,1,phi)*dbinom(0,1,phi)
  return(ll)
}

ggplot() + geom_function(fun=llfun2) + xlim(-5,8) + labs(x=expression(paste('logit(',phi,')')),y=expression(paste('Likelihood(H,H,T|',phi,')'))) + geom_vline(xintercept=logit(2/3),linetype='dashed')

```

::::

:::

## How do linear models fit into this?

::: columns

:::: column

- Usually we aren't interested in finding only a single parameter $\phi$.
- Solution: $\phi$ becomes a _linear_ function of the predictors

- Simple linear models take the form:
\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

- Generalized linear models are similar, except that:
1. Expected value ($\phi$) fed through a link function
2. Data is fit to a non-normal probability function

\vspace{12pt}

\begin{equation*}
\begin{split}
logit(\textcolor{orange}{\hat{\phi}}) & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
flips & \sim Binomial(\textcolor{orange}{\hat{\phi}})
\end{split}
\end{equation*}

::::

:::

\center Instead of finding $\phi$, __R finds the coefficients ($\textcolor{blue}{b_0}$, $\textcolor{blue}{b_1}$ ... $\textcolor{blue}{b_i}$) that create $\phi$__

## How do I fit GLMs in R?

Syntax and model output is very similar to `lm`

```{r, echo=TRUE, size='tiny'}
# y ~ x, where x is the predictor of y (~1 for just intercept)
mod_binomial <- glm(y2 ~ x1 + x2 , data = d1, family = 'binomial') #Fit a binomial GLM
summary(mod_binomial)
```

Dispersion and deviance will be discussed later...

## How do I get partial effects plots?

`crPlot` (from `car`) and `ggpredict` (`ggeffects`) work with fitted `glm` models

```{r, echo = TRUE, size='tiny', fig.height=2.5,fig.width=5}
ggpredict(mod_binomial, terms='x1 [all]') %>% #Partial effect of x1 term
  ggplot(aes(x, predicted)) + geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.3) +
  labs(x = 'x1', y = expression(paste('Chance of Success (',phi,')')))
```

## A challenger approaches!

- Dr. Roberto Darkley (Robert Barkley's evil nemesis) sent 2 people out to check out some bat roosts in Edmonton and Calgary. One of them dutifully counted bats at each roost, but the other one was really lazy, and just recorded "bats or no bats" (1 or 0).
- Fit a model to each of their data (found in `batDatGLM.csv`) using a GLM
  - `batCounts` should be modeled using a Poisson GLM, and `batPres` should use a Binomial GLM
  - Terms to include: `city` and `size` (no interaction)
- How do the models look? Compare the coefficients and see if they are different
  - Bonus: make a partial regression plot of terms in the Poisson GLM

## Model results

![](modResults.png)


