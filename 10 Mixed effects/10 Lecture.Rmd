---
title: "Mixed effects models"
subtitle: "Wheels within wheels"
author: "Samuel Robinson, Ph.D."
date: "December 10, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
---

```{r setup, include=FALSE}
#Trick to get smaller R code size with out resorting to LaTeX text sizes
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, warning=TRUE, size = 'footnotesize')
library(tidyverse)
theme_set(theme_classic())
library(lme4)
library(ggeffects)
library(ggpubr)
# library(knitr)
# library(kableExtra)
# library(latex2exp)

set.seed(123)

#Generate data
n <- 150
ngroups <- 15
x <- runif(n,-10,10) #Single fixed effect predictor
g <- sample(letters[1:ngroups],n,TRUE) #Groups
intercept <- 1
slopeX <- 0.5
sigmaR <- 3 #Residual sigma 
sigmaG <- 5 #Group intercept sigma
sigmaG_slope <- abs(slopeX*2) #Slope sigma (half slope value)

#Correlated intercepts and slopes, using Choleski matrices
raneffs <- matrix(rnorm(ngroups*2,0,1),ncol=2) #Uncorrelated unit normals
slopeCor <- 0.7 #Intercept-slope correlation
corMat <- matrix(c(1,slopeCor,slopeCor,1),ncol=2) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski transform of corMat
raneffs <- raneffs %*% cholCorMat #Induces correlation in slopes
raneffs <- raneffs * matrix(rep(c(sigmaG,sigmaG_slope),each=ngroups),ncol=2,
                            dimnames=list(letters[1:ngroups],c('Int','Slope'))) #Changes SD for each column
raneff_int <- model.matrix(~g-1) %*% raneffs[,1] #Intercept vector
raneff_slope <- x * model.matrix(~g-1) %*% raneffs[,2]  #Slope vector

yhat <- intercept + slopeX*x + raneff_int + raneff_slope  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data
dat <- data.frame(y,x,site=g) #Assemble into data frame

```

## Motivation

- What are mixed effects models?
  - Scary math (matrix algebra)
  - Variance partitioning
  - Fixed effects vs. Random effects
- Working with random effects
  - What should be a random effect?
  - Model validation
- Exercise

## What are mixed effects models?

Many different names:

1. Mixed effects models
2. Random effects models
3. Hierarchical models
4. Empirical/Bayesian hierarchical models
5. Latent variable models
6. Split-plot models\footnotemark

I like the term _heirarchical models_, as this is the closest to what I will teach you

\footnotetext{Earlier form of variance partitioning}

## Scary math

Unfortunately, we need a review of matrix algebra in order to explain this:

- This is a matrix: $$ A = \begin{bmatrix}
1 & 4 & 7 \\
2 & 5 & 8 \\
3 & 6 & 9
\end{bmatrix}
$$
  
- This is a vector $$ b = \begin{bmatrix}
1 & 2 & 3 
\end{bmatrix}
$$
  
- Multiplying them looks like this:

$$ A \times b = Ab = 1 \times \begin{bmatrix}
1  \\
2 \\
3 
\end{bmatrix} + 2 \times \begin{bmatrix}
4  \\
5  \\
6 
\end{bmatrix} + 3 \times \begin{bmatrix}
7  \\
8 \\
9 
\end{bmatrix} = \begin{bmatrix}
30  \\
36 \\
42 
\end{bmatrix}$$

## Why do we call them "linear models"?

Involves a _linear mapping_ of coefficients onto a model matrix

Coefficients: $$ \beta = \begin{bmatrix}
0.1 & 1.8 & -0.03 
\end{bmatrix}
$$

Model matrix:

$$ X = \begin{bmatrix}
1 & 1 & 10 \\
1 & 1 & 12 \\
1 & 0 & 9 \\
\vdots & \vdots & \vdots
\end{bmatrix}
$$

Multiplying them looks like: 

$$ \hat{y} = X\beta = 
\begin{bmatrix}
1.60  \\
1.54 \\
-0.17 \\
\vdots
\end{bmatrix}$$

## This is exactly what R does to fit models:

```{r, echo=TRUE, size='tiny'}
head(dat)
m1 <- lm(y~x,data=dat) #Use variable x to predict y
summary(m1)
```

## This is exactly what R does to fit models (cont.):

```{r, echo=TRUE, size='tiny'}
head(model.matrix(m1))
coef(m1)
pred2 <- model.matrix(m1) %*% coef(m1) #predicted = matrix * coefs
head(data.frame(pred1=predict(m1),pred2)) #same thing!
```

## Groups are coded by "dummy variables" (0s and 1s)

```{r, echo=TRUE, size='tiny'}
m2 <- lm(y~site,data=dat) #Use variable site to predict y
head(model.matrix(m2)) #0s and 1s used to identify groups
coef(m2) #This uses the 1st site as the "control" group
```

## Structure of LMs... now with matrices!

- All linear models take the form:
\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} = \textcolor{blue}{b_0}\textcolor{darkturquoise}{1} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $y$ is a vector of data you want to predict
- $\textcolor{orange}{\hat{y}}$ is a vector of _predicted values_ for $y$
- $\textcolor{darkturquoise}{X} = \textcolor{darkturquoise}{\{1,x_1...\}}$ is a matrix of _predictors_ for _y_
- $\textcolor{blue}{\beta} = \textcolor{blue}{\{b_0,b_1,...\}}$ is a vector of _coefficients_
- $y\sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})$ means:
  - "$y$ follows a Normal distribution with mean $\textcolor{orange}{\hat{y}}$ and SD $\textcolor{red}{\sigma}$"
  
## Fixed effects vs. Random effects

Say that $\textcolor{darkturquoise}{X}$ is a model matrix coding for 10 sites\footnotemark, and _y_ is something we're interested in predicting

::: columns

:::: column

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

\hfill\newline

- Site coefficients ($\textcolor{blue}{\beta}$) are unrelated to each other
- $\textcolor{red}{\sigma}$ is the SD of _residuals_
- Site is a __fixed effect__

::::

:::: column

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{darkturquoise}{X}\textcolor{purple}{\zeta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma}) \\
\textcolor{purple}{\zeta} & \sim Normal(0,\textcolor{red}{\sigma_{site}})
\end{split}
\end{equation*}

- Site coefficients ($\textcolor{blue}{\beta}$) are related to each other via a _Normal_ distribution
- $\textcolor{red}{\sigma}$ is the SD of _residuals_, $\textcolor{red}{\sigma_{site}}$ is the SD of _sites_
- Site is a __random effect__

::::

:::


\footnotetext{Intercept is a separate variable}

## Mixed effects = fixed + random effects

A mixed effects model has both __fixed__ and __random__ effects

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} + \textcolor{gray}{U}\textcolor{purple}{\zeta} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma}) \\
\textcolor{purple}{\zeta} & \sim Normal(0,\textcolor{red}{\sigma_{site}})
\end{split}
\end{equation*}

- $\textcolor{darkturquoise}{X}$ = fixed effects matrix (e.g. intercept, temperature)
- $\textcolor{blue}{\beta}$ = fixed effects coefficients
- $\textcolor{gray}{U}$ = random effects matrix (e.g. sites)
- $\textcolor{purple}{\zeta}$ = random effects coefficients
- $\textcolor{red}{\sigma}$, $\textcolor{red}{\sigma_{site}}$ = variance terms

## Mixed effect model example

Let's go back to our earlier example:

- We're interested in predicting _y_ using _x_ (fixed effects)
- Data was collected at a number of _sites_, which may affect _y_ "somehow"
- Effect of each site is normally distributed

```{r echo=TRUE, message=FALSE, warning=FALSE, size='tiny'}
head(dat)
library(lme4) #Mixed effects library 
mm1 <- lmer(y ~ x + (1|site),data=dat) #site is fit as "random intercepts"
```

## Mixed effect model example

::: columns

:::: column

```{r, echo=TRUE, size='tiny'}
summary(mm1)
```

::::

:::: column

Results from `lmer` model:

- Random effects:
  - _residual_ and _site_ variance ($\textcolor{red}{\sigma}$, $\textcolor{red}{\sigma_{site}}$)
- Fixed effects: 
  - Intercept and slope estimates ($\textcolor{blue}{\beta}$)
  - No d.f. and p-value \footnotemark 

::::

:::

\footnotetext{`lme4` author doesn't think they can be calculated. I largely agree}

## Mixed effect model results

- In a _random intercepts_ model, the regression line of _x_ on _y_ is allowed to move up or down around the main regression line for each site
- These changes in intercepts are _normally distributed_

```{r, fig.height=3, fig.width=5}
p1 <- dat %>% mutate(pred=predict(mm1),gpred=predict(mm1,re.form=~0)) %>% 
  ggplot(aes(x=x,y=y,col=site))+
  geom_point()+
  geom_line(aes(y=gpred),col='black',size=2)
p1 + geom_line(aes(y=pred))+guides(col=guide_legend(ncol=2))
```

## Mixed effect model results (cont.)

For plotting, we want a partial effects plot that marginalizes across sites (i.e. "What does the trend look like at the average site?")

- `ggpredict` works well for this. If you want partial residuals, you'll have to add them in yourself using `predict` and `residual`

```{r, fig.height=2, fig.width=4,warning=FALSE}
p2 <- ggpredict(mm1,terms='x') %>% data.frame() %>% #Gets predictions from mm1
  ggplot(aes(x=x))+ #Plots using ggplot
  geom_ribbon(aes(ymax=conf.high,ymin=conf.low),alpha=0.3) + #Intervals
  geom_line(aes(y=predicted))
ggarrange(p1+geom_line(aes(y=pred),show.legend = FALSE)+ylim(-20,20),
          p2+ylim(-20,20)+labs(y='y'),
          ncol=2,legend = 'none')
```

## Random slope + intercept

Suppose that _y_ wasn't just higher or lower at each site, but that the effect of _x_ on _y_ was higher or lower at each site

\begin{equation*}
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{darkturquoise}{X}\textcolor{blue}{\beta} + \textcolor{gray}{U}\textcolor{purple}{\zeta_{int}} + \textcolor{darkturquoise}{x_i}\textcolor{gray}{U}\textcolor{purple}{\zeta_{slope}} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma}) \\
\textcolor{purple}{\zeta_{int}} & \sim Normal(0,\textcolor{red}{\sigma_{int}}) \\
\textcolor{purple}{\zeta_{slope}} & \sim Normal(0,\textcolor{red}{\sigma_{slope}})
\end{split}
\end{equation*}

- $\textcolor{darkturquoise}{X}$ = fixed effects matrix (e.g. intercept, temperature)
- $\textcolor{darkturquoise}{x_i}$ = individual fixed effect (e.g. temperature)
- $\textcolor{blue}{\beta}$ = fixed effects coefficients
- $\textcolor{gray}{U}$ = random effects matrix (e.g. sites)
- $\textcolor{purple}{\zeta_{int}}, \textcolor{purple}{\zeta_{slope}}$ = random intercept and slope coefficients
- $\textcolor{red}{\sigma},\textcolor{red}{\sigma_{int}},\textcolor{red}{\sigma_{slope}}$ = variance terms

## Random slope and intercept example:

::: columns

:::: column

```{r, echo=TRUE, size='tiny'}
#Intercept varies with site, and slope of x can
#   also vary with site (both hierarchical)
mm2 <- lmer(y ~ x + (x|site),data=dat) 
summary(mm2)
```

::::

:::: column

\hfill\newline

Results from `lmer` model:

- Random effects:
  - _residual_, _slope_, and _site_ variance ($\textcolor{red}{\sigma}$, $\textcolor{red}{\sigma_{int}}$, $\textcolor{red}{\sigma_{slope}}$)
  - Correlation b/w intercept and slope = `r round(attr(VarCorr(mm2)[[1]],which='correlation')[1,2],2)`
    - Sites with higher intercept _also_ have a higher slope
- Fixed effects: 
  - Intercept and slope estimates
  
::::

:::

## Mixed effect model results

- Regression line of _x_ on _y_ is allowed to move up or down around the main regression line for each site (random intercepts)
- Slope of regression line can be more or less steep for each site (random slopes)
- Changes in intercepts and slopes are _normally distributed_, and in this example, are correlated with each other 

```{r, fig.height=3, fig.width=6}
dat %>% mutate(pred=predict(mm2),gpred=predict(mm2,re.form=~0)) %>% 
  ggplot(aes(x=x,y=y,col=site))+
  geom_point()+
  geom_line(aes(y=gpred),col='black',size=2) +
  geom_line(aes(y=pred))+guides(col=guide_legend(ncol=2))
```

<!-- ## Mixed effect model validation -->

<!-- - Similar to linear models, but we _also_ check whether the random intercepts are normally distributed -->
<!-- <!-- - If one site intercept is very different from the others, this is a clue for investigation! --> -->

<!-- ```{r} -->
<!-- par(mfrow=c(2,3)) -->
<!-- plot(fitted(mm2),resid(mm2,type='working'),xlab='Fitted values',ylab='Working residuals'); abline(h=0) -->
<!-- qqnorm(resid(mm2,type='working'),main='Residuals');qqline(resid(mm2,type='working')) -->
<!-- qqnorm(ranef(mm2)$site[,1],main='Random Intercepts');qqline(ranef(mm2)$site[,1]) -->
<!-- qqnorm(ranef(mm2)$site[,2],main='Random Slopes');qqline(ranef(mm2)$site[,2]) -->
<!-- plot(ranef(mm2)$site,xlab='Random Intercept',ylab='Random Slope',main='Intercept-Slope Correlation') -->
<!-- # par(mfrow=c(1,1)) -->
<!-- ``` -->

<!-- ## Hypothesis testing -->

<!-- Is this fixed effect important? (e.g. ANOVA) -->

<!-- - Use likelihood-based test via `drop1` (likelihood ratio test, AIC) -->
<!-- - Be careful to fit model with `REML = FALSE`! -->

<!-- ```{r, echo=TRUE, text='tiny'} -->
<!-- mm1 <- update(mm1,REML=FALSE) #Refit model using ML rather than REML -->
<!-- drop1(mm1,test='Chisq') #x has a very strong effect! -->
<!-- ``` -->

<!-- ## Hypothesis testing (cont.) -->

<!-- How do I know this effect is different from _x_? -->
<!-- - Use Wald Z-test (2-sided p-value from Z-test) -->

<!-- ```{r} -->
<!-- mm1 <- update(mm1,REML=TRUE) #Reset to REML -->
<!-- meanEst <- fixef(mm1)[2] #Get mean -->
<!-- seEst <- sqrt(vcov(mm1)[2,2]) #Get standard error -->
<!-- (1-pnorm(meanEst/seEst,0,1))*2 #p-value from 2-sided Z-test -->
<!-- ``` -->

<!-- - `glht` from `library(multcomp)` works with `lmer` models if you are comparing between coefficients (e.g. "Is treatment A different from B and C?") -->

## Why do we need to do any of this?

_"My supervisor told me to just use site as a fixed effect. Why can't I do that?"_ 

- You can do it this way, but you may encounter the following problems:
  - You lose the _partial pooling_ that occurs in mixed effects models = Worse estimates of site effects!
  - You lose 1 d.f. for each site = Type II error $\uparrow$ = You may not find the fixed effect of interest, even if it's there!
  - Sites with low sample sizes may cause your models to break
  - People\footnotemark who have read statistics books published after 1980 may ask questions
- However, if you have a low number of sites (1-10), fixed effects may work better
  - Hard to estimate $\sigma_{site}$ if number of sites is low
  - Partial pooling doesn't really help if no other sites are available to "borrow strength" from
  - Easier to interpret (p-values, ANOVA, etc.)
  
\footnotetext{e.g. me}