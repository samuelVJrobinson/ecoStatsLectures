---
title: "Linear models 2"
subtitle: "More bells and whistles"
author: "Samuel Robinson, Ph.D."
date: "October 15, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  \useinnertheme{circles}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)

set.seed(123)

#Fit models
modp1 <- lm(mpg~disp,data=arrange(mtcars,disp))
modp2 <- lm(mpg~factor(gear),data=arrange(mtcars,am,disp))
modp3 <- lm(mpg~disp+factor(gear),data=arrange(mtcars,am,disp))
modp4 <- lm(mpg~disp*factor(gear),data=mtcars)

#Plots

#Continuous
p1 <- ggplot(arrange(mtcars,disp),aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Categorical - 3 levels
p2 <- ggplot(arrange(mtcars,gear,mpg),aes(x=factor(gear),y=mpg))+ 
  geom_point(position=position_jitter(width=0.05))+labs(x='gears')+
  geom_point(aes(y=mpg),stat='summary',fun=mean,col='orange',size=3) #Mean only

#Continuous + categorial
p3 <- ggplot(cbind(modp3$model,pred=predict(modp3)),aes(x=disp,y=mpg,col=`factor(gear)`))+geom_point()+labs(col='gears')+
  geom_line(aes(y=pred))+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Interaction
p4 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(gear)))+geom_point()+labs(col='gears')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

```


## Motivation {.build}

> - _I have 2+ groups of data, and I want to know whether the means are different_

> - _I have 2+ groups of bivariate data, and I want to know whether the relationships differ between groups_

```{r examplePlots, echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
ggarrange(p2,p4,ncol=2) #Display mtcars data
```

## Categorial data, 3 categories

::: columns

:::: column

```{r, fig.height=3, fig.width=3}

b0Lab <- paste('b[0] == ',round(coef(modp2)[1],2))
b1Lab <- paste('b[1] == ',round(coef(modp2)[2],2))
b2Lab <- paste('b[2] == ',round(coef(modp2)[3],2))


  p2 + annotate('text', x=1.5, y = coef(modp2)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
    geom_hline(yintercept=coef(modp2)[1],linetype='dashed',col='blue') +
    
    annotate('text', x=2.35, y = (coef(modp2)[1] + coef(modp2)[2])*1.05, col='blue', label=b1Lab, parse=TRUE)+
    annotate('segment', x = 2.25, xend = 2.25, y= coef(modp2)[1], yend = coef(modp2)[1] + coef(modp2)[2],linetype='dashed',col='blue') +
    
    annotate('text', x=3.35, y = (coef(modp2)[1] + coef(modp2)[3])*1.05, col='blue', label=b2Lab, parse=TRUE)+
    annotate('segment', x = 3.25, xend = 3.25, y= coef(modp2)[1], yend = coef(modp2)[1] + coef(modp2)[3],linetype='dashed',col='blue') +
    
    theme(axis.title.x.bottom=element_text(colour='darkturquoise'),
            axis.text.x.bottom=element_text(colour='darkturquoise'))
```

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_5} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

The more factor levels, the more coefficients:

>- $mpg$ is the thing you're interested in predicting
>- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
>- $\textcolor{darkturquoise}{gear}$ is the _predictor_ of _mpg_
  >- set of 0s and 1s
  >- $\textcolor{darkturquoise}{gears_4}=$ "is this data point from a 4-gear car?"
>- $\textcolor{blue}{b_0}=$ _intercept_
>- $[\textcolor{blue}{b_1},\textcolor{blue}{b_2}]=$ are _coefficients_ for $\textcolor{darkturquoise}{gears}$

::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#Formula structure: y ~ x
mod1 <- lm(mpg ~ factor(gear), #mpg depends on gears
           data = mtcars) #Name of the dataframe containing mpg & gears
summary(mod1)
```

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod1Matrix <- model.matrix(mod1) #Get model matrix (columns used to predict mpg)
head(mod1Matrix,28) #Show first 28 rows of model matrix
```

## What about if 2 things are both important?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
p3
```

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
&+ \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_3}\textcolor{darkturquoise}{gears_5} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

>- Suppose that both $\textcolor{darkturquoise}{disp}$ and $\textcolor{darkturquoise}{gears}$ are important for predicting $mpg$?
>- This is very similar to the last example, except that now we've added $\textcolor{darkturquoise}{disp}$
>- $\textcolor{darkturquoise}{gears}$ now changes the intercept, while $\textcolor{darkturquoise}{disp}$ changes the slope of all the lines 
>- Does it look like $\textcolor{darkturquoise}{gear}$ is very important?

::::

:::


## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#mpg depends on disp and gears
mod2 <- lm(mpg ~ disp+factor(gear), data = mtcars) 
summary(mod2)
```

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod2Matrix <- model.matrix(mod2) #Get model matrix (columns used to predict mpg)
colnames(mod2Matrix) <- gsub('factor\\(gear\\)','gear',colnames(mod2Matrix)) #Shorten colnames
head(mod2Matrix,28) #Show first 28 rows of model matrix
```

## Interlude: problems with plotting raw data

>- Say that I've fit the following model:  
`mpg ~ disp + factor(gear)`
>- All of the plots below are using raw data, but which one is "telling the truth"?
>- Answer: __c__. _a_ and _b_ are hiding the effect of the other variable

```{r, echo=FALSE, fig.height=3, fig.width=8} 
ggarrange(p1,p2,p3,ncol=3,labels=letters[1:3])
```

## How do I plot these model results?

::: columns

:::: column

Rule for plotting model results:  

1. If the model uses `N` variables, you should show all `N` effects _simultaneously_
2. If this is impractical, you should use a __partial effects plot__

Other names for partial effects:

- _counterfactual_ plot, _predictor effect_ plot, _leverage_ plot
- Try using \texttt{effects} or \texttt{ggeffects}. Requires the \texttt{effects} and \texttt{ggeffect} packages

```{r include=FALSE}
n <- 100; ncoef <- 5 #Number of data points and coefficients
pred <- data.frame(replicate(n=ncoef,expr=rnorm(n,0,2))) #Predictor matrix
beta <- runif(ncoef,-5,5) #Coefficients
yhat <- as.matrix(pred) %*% beta #Predicted value
y <- rnorm(n,yhat,2) #SD=2
pred$y <- y
```

::::

:::: column

Incorrect example, using raw data:
\tiny
```{r echo=TRUE}
#Fit model with 5 variables (all important)
simMod <- lm(y~X1+X2+X3+X4+X5,data=pred) 
#Incorrect way, using raw data 
plot(y~X5,data=pred,pch=19,cex.lab=3) 
```

\normalsize
The effect of \emph{X5} is actually \textbf{very} strong ($p<0.0001$), but it doesn't look like it from this plot!
::::

:::

## Partial effects plots - using _effects_
\tiny
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=8,fig.height=5}
library(effects) #Load effects package
simModEff <- predictorEffects(simMod,partial.residuals=TRUE) #Calculate partial effects
#Plot partial effects
plot(simModEff,lines=list(col='red'), partial.residuals=list(pch=19,col='black',cex=0.25))
```

## Partial effects plots - using _ggpredict_
\tiny
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=8,fig.height=5}
library(ggeffects) #Load ggeffects package
simModEff2 <- ggeffect(simMod,terms=c('X5')) #Calculate partial effects for X5
plot(simModEff2) #Plot effect of X5
```

## Interactions

What if the slopes _and_ intercepts differ between groups?

```{r, fig.height=3, fig.width=5}
  p4 
```

## Interactions

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
b0Lab <- paste('b[0] == ',round(coef(modp4)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp4)[2],3))
b4Lab <- paste('b[4] == ',round(coef(modp4)[4],3))
b5Lab <- paste('b[5] == ',round(coef(modp4)[6],3)) 

p4 + xlim(0,NA) +
  #Text/lines for gear==3
  annotate('text', x=250, y = coef(modp4)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
  annotate('segment', x = 400, xend = 400, y= coef(modp4)[c(1,2)] %*% c(1,400), yend = (coef(modp4)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') +
  annotate('segment', x = 300, xend = 400, y= (coef(modp4)[c(1,2)] %*% c(1,400))*1.2, yend = (coef(modp4)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') + 
  annotate('text', x=400, y = (coef(modp4)[c(1,2)] %*% c(1,400))*1.25, col='blue', label=b1Lab, parse=TRUE) +
  geom_hline(yintercept=coef(modp4)[1],linetype='dashed',col='blue') +
  geom_abline(intercept=coef(modp4)[1],slope=coef(modp4)[2],col='blue',linetype='dashed') +
  #Text/lines for gear==5
  annotate('text', x=250, y = sum(coef(modp4)[c(1,4)])*1.05, col='red', label=b4Lab, parse=TRUE) +
  annotate('segment', x = 250, xend = 250, y= coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250), 
           yend = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('segment', x = 180, xend = 250, y= (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2,
           yend = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('text', x = 350, y = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, col='red', label=b5Lab, parse=TRUE) +
  geom_hline(yintercept=sum(coef(modp4)[c(1,4)]),linetype='dashed',col='red') +
  geom_abline(intercept=sum(coef(modp4)[c(1,4)]),slope=sum(coef(modp4)[c(2,6)]),col='red',linetype='dashed')
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp}\\
& + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_3}\textcolor{darkturquoise}{gears_5}\\
& + \textcolor{blue}{b_4}\textcolor{darkturquoise}{(disp\times gears_4)}\\
& + \textcolor{blue}{b_5}\textcolor{darkturquoise}{(disp\times gears_5)}\\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

>- Interactions occur when predictors are _multiplied_
>- In this case, $\textcolor{darkturquoise}{disp}$ is multiplied by $\textcolor{darkturquoise}{gears_4}$ and $\textcolor{darkturquoise}{gears_5}$
>- $\textcolor{darkturquoise}{gears}$ now changes the intercept and the slope of the relationship between $mpg$ and $\textcolor{darkturquoise}{disp}$
::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#mpg depends on disp interacted (*) with gears
mod2 <- lm(mpg ~ disp*factor(gear), data = mtcars) 
summary(mod2)
```
\small
Beware of fitting too many interactions, or else the \emph{Bilbo effect} occurs!

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod2Matrix <- model.matrix(mod2) #Get model matrix (columns used to predict mpg)
colnames(mod2Matrix) <- gsub('factor\\(gear\\)','gear',colnames(mod2Matrix)) #Shorten colnames
head(mod2Matrix,28) #Show first 28 rows of model matrix
```

## A challenger approaches!

- Since you're all bat folks, here's some bat data!
  - `batDat.csv`
- Data: 100 bat weights from 2 cities, recorded along with sex and age
- How do these variables affect bat weight? 
  - Think about how these variables might be related to weight using your `brain`
  - Fit a model using `lm`
  - Make some plots, using `effects` or `ggeffects`
