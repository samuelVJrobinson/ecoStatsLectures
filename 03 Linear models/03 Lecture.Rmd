---
title: "Linear models"
subtitle: "Modeling... linearly!"
author: "Samuel Robinson, Ph.D."
date: "Sep. 22, 2023"
output: 
  beamer_presentation:
    incremental: true
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
    fig_caption: false
urlcolor: blue
df_print: kable
classoption: aspectratio=169
header-includes: 
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
  - \let\oldShaded\Shaded %Change fontsize of R code chunks
  - \let\endoldShaded\endShaded
  - \renewenvironment{Shaded}{\scriptsize\oldShaded}{\endoldShaded}
  - \let\oldverbatim\verbatim %Change fontsize of code chunk output
  - \let\endoldverbatim\endverbatim
  - \renewenvironment{verbatim}{\tiny\oldverbatim}{\endoldverbatim}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
custTheme <- theme_classic()+
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1),'cm'))
theme_set(custTheme)
library(ggpubr)
library(knitr)
library(kableExtra)

set.seed(123)

#Fit models for later use
modp1 <- lm(mpg~disp,data=arrange(mtcars,disp)) # Simple linear - continuous predictor
modp2a <- lm(mpg~am,data=arrange(mtcars,am,disp)) # Simple linear - categorial predictor with 2 levels
modp2b <- lm(mpg~factor(gear),data=arrange(mtcars,am,disp)) # Simple linear - categorial predictor with 3 levels
modp3 <- lm(mpg~disp+factor(gear),data=arrange(mtcars,am,disp)) # Multiple linear - continuous and categorial predictor
modp4 <- lm(mpg~disp*factor(gear),data=mtcars) # Multiple linear - continuous, categorial, and interaction

#Plots of mtcars data

#Continuous
p1 <- ggplot(arrange(mtcars,disp),aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Categorical - 2 levels
p2a <- ggplot(arrange(mtcars,am,disp),aes(x=factor(am,labels=c('auto','manual')),y=mpg))+  
  geom_point(position=position_jitter(width=0.05))+labs(x='am')+
  geom_point(aes(y=mpg),stat='summary',fun=mean,col='orange',size=3) #Mean only
  
#Categorical - 3 levels
p2b <- ggplot(arrange(mtcars,gear,mpg),aes(x=factor(gear),y=mpg))+ 
  geom_point(position=position_jitter(width=0.05))+labs(x='gears')+
  geom_point(aes(y=mpg),stat='summary',fun=mean,col='orange',size=3) #Mean only

#Continuous + categorial
p3 <- ggplot(cbind(modp3$model,pred=predict(modp3)),aes(x=disp,y=mpg,col=`factor(gear)`))+geom_point()+labs(col='gears')+
  geom_line(aes(y=pred))+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Interaction
p4 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(gear)))+geom_point()+labs(col='gears')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

```

## Outline

::: columns

:::: column

- What are linear models? How do I fit them?

- Making sure the model is working properly

- Plotting and interpreting model results

- How to think about models

:::: 

:::: column

![](xkcd_extrapolating.png){width=100%}

::::

:::

# Part 1: How do they work?


## Motivation

>- _I measured 2 things and I want to know if they're related to each other_

>- _I have groups of data, and I want to know whether the means are different_

```{r examplePlots, echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
ggarrange(p1,p2a,ncol=2) #Display mtcars data
```

## Terminology 

Linear models go by many different names. All these models are all doing _exactly the same thing_:

- Linear regression

- Least-squares regression

- Simple linear model (SLM)

- Multiple linear model/regression

- Analysis of Variance (ANOVA)

- Analysis of Covariance (ANCOVA)

\pause

I use a set of terminology that I find very helpful, from [Berliner (1996)](https://link.springer.com/chapter/10.1007/978-94-011-5430-7_3). I'll be using it here, as well as for describing more complex models.

## Model terminology

All linear models take the form:
\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x_1} + \textcolor{blue}{b_2}\textcolor{darkturquoise}{x_2} ... + \textcolor{blue}{b_i}\textcolor{darkturquoise}{x_i} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

\pause

- $y$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{y}}$ is the _predicted value_ of $y$
- $\textcolor{darkturquoise}{x_1...x_i}$ are _predictors_ of _y_
- $\textcolor{blue}{b_1...b_i}$ are _coefficients_ for each predictor $\textcolor{darkturquoise}{x_i}$
- $\textcolor{blue}{b_0}$ is the _intercept_, a coefficient that doesn't depend on predictors
- $y\sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})$ means:
  - "$y$ follows a Normal distribution with mean $\textcolor{orange}{\hat{y}}$ and SD $\textcolor{red}{\sigma}$"

\pause
This may look terrifying, but let's use a simple example:

## Example

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
meanDisp <- mean(mtcars$disp) #Mean displacement
y_meanDisp <- coef(modp1) %*% c(1,meanDisp) #Predicted y at mean displacement
b0Lab <- paste('b[0] == ',round(coef(modp1)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp1)[2],3)) 

p1 + xlim(0,NA) + 
  geom_hline(yintercept=coef(modp1)[1],linetype='dashed',col='blue') +
  geom_abline(intercept=coef(modp1)[1],slope=coef(modp1)[2],col='orange',linetype='dashed') +
  annotate('text', x=meanDisp, y = coef(modp1)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
  annotate('segment', x = meanDisp, xend = meanDisp, y= y_meanDisp, yend = y_meanDisp*1.2,linetype='dashed',col='blue') +
  annotate('text', x=meanDisp*1.05, y = y_meanDisp*1.25, col='blue', label=b1Lab, parse=TRUE) +
  annotate('segment', x = (y_meanDisp*1.2-coef(modp1)[1])/coef(modp1)[2] , xend = meanDisp, 
           y= y_meanDisp*1.2 ,yend = y_meanDisp*1.2,linetype='dashed',col='blue') +
  theme(axis.title.x.bottom=element_text(colour='darkturquoise'),
          axis.text.x.bottom=element_text(colour='darkturquoise'))+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $mpg$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
- $\textcolor{darkturquoise}{disp}$ is the _predictor_ of _mpg_
- $\textcolor{blue}{b_0}$ is the _intercept_, $\textcolor{blue}{b_1}$ is the _coefficient_ (slope) for $\textcolor{darkturquoise}{disp}$
- $mpg\sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})$ means:
  - "$mpg$ follows a Normal distribution with mean $\textcolor{orange}{\hat{mpg}}$ and SD $\textcolor{red}{\sigma}$"
- $\textcolor{red}{\sigma}$ isn't displayed on the figure. Where is it?

::::

:::


## Example (cont.)

$\textcolor{red}{\sigma}$ isn't displayed on the figure. Where is it?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}

modp1_sigma <- summary(modp1)$sigma #Residual SD

d1 <- data.frame(disp=sort(mtcars$disp),pred=predict(modp1)) %>% #Predictions + residual SD
  mutate(upr1=pred+modp1_sigma,lwr1=pred-modp1_sigma) %>% 
  mutate(upr2=pred+modp1_sigma*2,lwr2=pred-modp1_sigma*2) 

#Create sideways normal curves
curveDf <- data.frame(disp=c(150,350), mpg=coef(modp1)[1]+coef(modp1)[2]*c(150,350))

#Modified from https://stackoverflow.com/questions/62551795/make-ggplot-with-regression-line-and-normal-distribution-overlay
mkCurve <- function(x,y,xrange,yrange){
  mu <- y
  rg <- mu + yrange
  seq <- seq(rg[1], rg[2], length.out = 100)
  d <- data.frame(
    disp = dnorm(seq, mean = mu, sd = 3.2) + x,
    mpg = seq
  )
  #Scale x-axis to xrange
  d$disp <- min(d$disp)+(d$disp-min(d$disp))*xrange/c(max(d$disp)-min(d$disp))
  return(d)
}

curveDf <- with(curveDf,mapply(mkCurve,x=disp,y=mpg,MoreArgs=list(xrange=50,yrange=c(-10,10)),SIMPLIFY=FALSE)) %>% 
  bind_rows(.id='ID')

p1 + 
  # geom_ribbon(data=d1,aes(x=disp,y=NULL,ymax=upr2,ymin=lwr2),fill='red',alpha=0.2)+
  # geom_ribbon(data=d1,aes(x=disp,y=NULL,ymax=upr1,ymin=lwr1),fill='red',alpha=0.2)+
  geom_path(data=curveDf,aes(group=ID),col='red')+
  geom_vline(xintercept = c(150,350),linetype='dashed')
```

::::

:::: column

- $\textcolor{red}{\sigma}$ is the "leftover" or "residual" variance
- i.e. variation between samples that the model couldn't explain
- Since $y\sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})$, this means that points are normally distributed around the _entire line_ of $\textcolor{orange}{\hat{y}}$
- If you took a vertical slice at each part of the x-axis, the distribution would be _Normal_

::::

:::

## How do I get R to fit this model?

`lm` is one of the main functions used for linear modeling:

```{r, echo=TRUE} 
#Formula= y ~ x, data = Name of the dataframe containing mpg & disp
mod1 <- lm(mpg ~ disp, data = mtcars); summary(mod1)
```

For a detailed breakdown of `lm`'s output, click [here](https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output)

## Simulate data

Now that we know how linear models work, we can simulate our own data:

:::{.columns}

::::{.column width="50%"}
\scriptsize

```{r, echo=TRUE, out.width="100%"}
#Parameters:

b0 <- 1 #Intercept
b1 <- 2 #Slope 
sigma <- 3 #SD

#Make up some data:

x <- 0:30 #Predictor values

#Predicted y values
pred_y <- b0 + b1*x 

#Add "noise" around pred_y
actual_y <- rnorm(n = length(pred_y),
                  mean = pred_y,
                  sd= sigma)

```
::::

::::{.column width="50%"}
\scriptsize
```{r, echo=TRUE, fig.width=5,fig.height=4}
#Plot the data we just made
plot(x,pred_y,col='orange',pch=19,type='l',
     ylab='y') 
points(x,actual_y,col='black',pch=19)
```

::::

:::

## Fit a model from simulated data

How does R do at finding the coefficients? Remember: $b_0 = 1, b_1 = 2, \sigma = 3$

```{r, echo=TRUE, fig.width=5,fig.height=5}
fakeDat <- data.frame(x = x, y = actual_y, pred = pred_y) #Simulated data in a dataframe 
mod1sim <- lm(y ~ x, data = fakeDat); summary(mod1sim) #Fit model
```

## What about categorical data?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}

b0Lab <- paste('b[0] == ',round(coef(modp2a)[1],2))
b1Lab <- paste('b[1] == ',round(coef(modp2a)[2],2)) 

  p2a +
    annotate('text', x=1.5, y = coef(modp2a)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
    geom_hline(yintercept=coef(modp2a)[1],linetype='dashed',col='blue') +
    
    annotate('text', x=2.35, y = (coef(modp2a)[1] + coef(modp2a)[2])*1.05, col='blue', label=b1Lab, parse=TRUE)+
    annotate('segment', x = 2.25, xend = 2.25, y= coef(modp2a)[1], yend = coef(modp2a)[1] + coef(modp2a)[2],linetype='dashed',col='blue') +
    
    theme(axis.title.x.bottom=element_text(colour='darkturquoise'),
            axis.text.x.bottom=element_text(colour='darkturquoise'))
```


::::

:::: column

This uses _exactly the same_ math!

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{am} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $mpg$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
- $\textcolor{darkturquoise}{am}$ is the _predictor_ of _mpg_
  - set of 0s and 1s, not continuous
- $\textcolor{blue}{b_0}$ is the _intercept_, $\textcolor{blue}{b_1}$ is the _coefficient_ for $\textcolor{darkturquoise}{am}$
- Where is $\textcolor{red}{\sigma}$?

::::

:::

## How do I get R to fit this model?

Syntax is exactly the same for this model

\tiny
```{r, echo=TRUE} 
#Formula structure: y ~ x
mod2 <- lm(mpg ~ am, #mpg depends on am
           data = mtcars) #Name of the dataframe containing mpg & am
summary(mod2)
```

## First challenge

- Simulate some data with 2 (discrete) levels. My suggestion:
  - ~~Steal~~Borrow my code, and change the predictor from continuous to discrete
  - Useful command: `rep` (replicate)
    - e.g. `rep(x=c(0,1),each=10)`
  - Useful command: `rnorm` (generate normally-distributed data)
    - e.g. `rnorm(n=100,mean=0,sd=1)`
- Use `lm` to fit a model to the data you just simulated
  - How does R do at guessing your coefficients?
  
  
## Modeling philosophy

All parametric models are approximating a __generative process__ 

\pause

When we're fitting the model `lm(y ~ x)`, our implicit model of the process is:

- "I think that there is some kind of _average value_ of __y__, that I'll call $\textcolor{orange}{\hat{y}}$" (parametric)
- "I think that $\textcolor{orange}{\hat{y}}$ responds to __x__ in a __straight line__" (linear in parameters)
- "I think that $\textcolor{orange}{\hat{y}}$ is spitting out __normally distributed__ data around it" (normal residuals)
- "I think that __y__ values aren't affected by the other __y__ values that come before them" (independence)
- "I think that the normal distribution is _about the same_ anywhere along $\textcolor{orange}{\hat{y}}$" (stationary)

\pause 

All of these can be changed, as we'll see during the following weeks!

## Modeling philosophy (cont.)

::: columns

:::: column

\vspace{0.5cm}

- When we gather data, we're seeing the outcome of this generative process, and trying to guess what the underlying process is.

- __It is up to us to think about what this process might be like.__

\pause

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{y}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{x} \\
y & \sim Normal(\textcolor{orange}{\hat{y}},\textcolor{red}{\sigma})\\
\textcolor{blue}{b_0} = 1, \textcolor{blue}{b_1} = & 2, \textcolor{red}{\sigma} = 3 \text{ : "True" values} \\
\textcolor{blue}{\hat{b_0}} = 2.0, \textcolor{blue}{\hat{b_1}} = & 1.9, \textcolor{red}{\hat{\sigma}} = 2.9 \text{ : Estimated values} \\
\end{split}
\end{equation*}

::::

:::: column

```{r, echo=FALSE, fig.width=4,fig.height=5}
plot(x,pred_y,col='orange',pch=19,type='l',ylab='y') 
abline(2.02,1.93,col='orange',lty=2)
points(x,actual_y,col='black',pch=19)
```
::::

:::


# Part 2: More bells and whistles

## Motivation

> - _I have 2+ groups of data, and I want to know whether the means are different_

> - _I have 2+ groups of bivariate data, and I want to know whether the relationships differ between groups_

```{r examplePlots2, echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
ggarrange(p2b,p4,ncol=2) #Display mtcars data
```

## Categorical data, 3 categories

::: columns

:::: column

```{r, fig.height=3, fig.width=3}

b0Lab <- paste('b[0] == ',round(coef(modp2b)[1],2))
b1Lab <- paste('b[1] == ',round(coef(modp2b)[2],2))
b2Lab <- paste('b[2] == ',round(coef(modp2b)[3],2))


  p2b + annotate('text', x=1.5, y = coef(modp2b)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
    geom_hline(yintercept=coef(modp2b)[1],linetype='dashed',col='blue') +
    
    annotate('text', x=2.35, y = (coef(modp2b)[1] + coef(modp2b)[2])*1.05, col='blue', label=b1Lab, parse=TRUE)+
    annotate('segment', x = 2.25, xend = 2.25, y= coef(modp2b)[1], yend = coef(modp2b)[1] + coef(modp2b)[2],linetype='dashed',col='blue') +
    
    annotate('text', x=3.35, y = (coef(modp2b)[1] + coef(modp2b)[3])*1.05, col='blue', label=b2Lab, parse=TRUE)+
    annotate('segment', x = 3.25, xend = 3.25, y= coef(modp2b)[1], yend = coef(modp2b)[1] + coef(modp2b)[3],linetype='dashed',col='blue') +
    
    theme(axis.title.x.bottom=element_text(colour='darkturquoise'),
            axis.text.x.bottom=element_text(colour='darkturquoise'))
```

::::

:::: column

The more factor levels, the more coefficients:

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_5} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- $mpg$ is the thing you're interested in predicting
- $\textcolor{orange}{\hat{mpg}}$ is the _predicted value_ of $mpg$
- $\textcolor{darkturquoise}{gear}$ is the _predictor_ of _mpg_
  - set of 0s and 1s
  - $\textcolor{darkturquoise}{gears_4}=$ "is this data point from a 4-gear car?"
- $\textcolor{blue}{b_0}=$ _intercept_ (first level of `gear` factor)
- $[\textcolor{blue}{b_1},\textcolor{blue}{b_2}]=$ are _coefficients_ for $\textcolor{darkturquoise}{gears}$

::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#Formula structure: y ~ x
mod1 <- lm(mpg ~ factor(gear), #mpg depends on gears
           data = mtcars) #Name of the dataframe containing mpg & gears
summary(mod1)
```

## Dummy variables
\tiny
```{r, echo=TRUE} 
mod1Matrix <- model.matrix(mod1) #Get model matrix (columns used to predict mpg)
head(mod1Matrix,20) #Show first 20 rows of model matrix
```

## What if 2 things are _both_ important?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
p3
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
&+ \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_3}\textcolor{darkturquoise}{gears_5} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}


- Suppose that both $\textcolor{darkturquoise}{disp}$ and $\textcolor{darkturquoise}{gears}$ are important for predicting $mpg$?
- This is very similar to the last example, except that now we've added $\textcolor{darkturquoise}{disp}$
- $\textcolor{darkturquoise}{gears}$ now changes the intercepts, while $\textcolor{darkturquoise}{disp}$ changes the overall slope
- Now that both variables are included, does it look like $\textcolor{darkturquoise}{gear}$ is very important?

::::

:::


## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#mpg depends on disp and gears
mod2 <- lm(mpg ~ disp+factor(gear), data = mtcars) 
summary(mod2)
```

## Dummy variables

\tiny
```{r, echo=TRUE} 
mod2Matrix <- model.matrix(mod2) #Get model matrix (columns used to predict mpg)
head(mod2Matrix,20) #Show first 20 rows of model matrix
```

## Second challenge 

- You all brought some of your own data... didn't you??

- Make a simple model of your data! Choose a _numeric_ variable to predict, and some other variables that might be good at predicting it. Fit a model and see what it says

- `lm` model input:

\pause

`model1 <- lm(y ~ x1 + x2 + ..., data = myDataFrame)`

`summary(model1)` 


## Interlude: problems with plotting raw data

- Say that I've fit the following model:  
`mpg ~ disp + factor(gear)`
- All of the plots below are using raw data, but which one is "telling the truth"?
- Answer: __c__. _a_ and _b_ are hiding the effect of the other variable

\pause

```{r, echo=FALSE, fig.height=3, fig.width=8} 
ggarrange(p1,p2b,p3,ncol=3,labels=letters[1:3])
```

## How do I plot these model results?

::: columns

:::: column

Rules for plotting model results with $>1$ terms:  

\pause 

1. If the model uses `N` terms, you should show all `N` effects _simultaneously_
2. If this is impossible, you should use a __partial effects plot__

\pause

Other names for partial effects:

- _counterfactual_ plot, _predictor effect_ plot, _leverage_ plot
- Try using \texttt{effects} or \texttt{ggeffect} packages for making these plots

```{r echo=FALSE, eval=TRUE}
n <- 100; ncoef <- 5 #Number of data points and coefficients
pred <- data.frame(replicate(n=ncoef,expr=rnorm(n,0,2))) #Predictor matrix
beta <- runif(ncoef,-5,5) #Coefficients
yhat <- as.matrix(pred) %*% beta #Predicted value
y <- rnorm(n,yhat,2) #SD=2
pred$y <- y
```

::::

:::: column

\pause

Incorrect example, using raw data:
\tiny
```{r echo=TRUE, fig.height=4, fig.width=6}
#Fit model with 5 variables (all important)
simMod <- lm(y~X1+X2+X3+X4+X5,data=pred) 
#Plot x5 and y
plot(y~X5,data=pred,pch=19,cex.lab=3) 
```

\vspace{-0.5cm}

::::

:::

\pause

\normalsize
The effect of \emph{X5} is actually \textbf{very} strong ($p<0.0001$), but it doesn't look like it from this plot!


## Partial effects plots - using _effects_
\tiny
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=8,fig.height=4}
library(effects) #Load effects package
simModEff <- predictorEffects(simMod,partial.residuals=TRUE) #Calculate partial effects
#Plot partial effects
plot(simModEff,lines=list(col='red'), partial.residuals=list(pch=19,col='black',cex=0.25))
```

## Partial effects plots - using _ggeffects_

::: columns

:::: column
```{r, ggeffectsPlot, echo=TRUE, eval=FALSE}
#Load ggeffects package
library(ggeffects) 

#Calculate partial effects for X5
simModEff2 <- ggpredict(simMod,terms=c('X5')) 

#Plot the effect of X5
plot(simModEff2,residuals=TRUE) 
```

- You can also turn `ggeffect` objects into a dataframe and make your own custom plots

::::

:::: column

```{r, ggeffectsPlot, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width=4,fig.height=4}
```

::::

:::


## Interactions

What if the slopes _and_ intercepts differ between groups?

```{r, fig.width=8,fig.height=4}
  p4 
```

## Interactions

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
b0Lab <- paste('b[0] == ',round(coef(modp4)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp4)[2],3))
b4Lab <- paste('b[4] == ',round(coef(modp4)[4],3))
b5Lab <- paste('b[5] == ',round(coef(modp4)[6],3)) 

p4 + xlim(0,NA) +
  #Text/lines for gear==3
  annotate('text', x=250, y = coef(modp4)[1]*1.05, col='blue', label=b0Lab, parse=TRUE) +
  annotate('segment', x = 400, xend = 400, y= coef(modp4)[c(1,2)] %*% c(1,400), yend = (coef(modp4)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') +
  annotate('segment', x = 300, xend = 400, y= (coef(modp4)[c(1,2)] %*% c(1,400))*1.2, yend = (coef(modp4)[c(1,2)] %*% c(1,400))*1.2,linetype='dashed',col='blue') + 
  annotate('text', x=400, y = (coef(modp4)[c(1,2)] %*% c(1,400))*1.25, col='blue', label=b1Lab, parse=TRUE) +
  geom_hline(yintercept=coef(modp4)[1],linetype='dashed',col='blue') +
  geom_abline(intercept=coef(modp4)[1],slope=coef(modp4)[2],col='blue',linetype='dashed') +
  #Text/lines for gear==5
  annotate('text', x=250, y = sum(coef(modp4)[c(1,4)])*1.05, col='red', label=b4Lab, parse=TRUE) +
  annotate('segment', x = 250, xend = 250, y= coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250), 
           yend = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('segment', x = 180, xend = 250, y= (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2,
           yend = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, linetype='dashed',col='red') +
  annotate('text', x = 350, y = (coef(modp4)[c(1,2,4,6)] %*% c(1,250,1,250))*1.2, col='red', label=b5Lab, parse=TRUE) +
  geom_hline(yintercept=sum(coef(modp4)[c(1,4)]),linetype='dashed',col='red') +
  geom_abline(intercept=sum(coef(modp4)[c(1,4)]),slope=sum(coef(modp4)[c(2,6)]),col='red',linetype='dashed')
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp}\\
& + \textcolor{blue}{b_2}\textcolor{darkturquoise}{gears_4} + \textcolor{blue}{b_3}\textcolor{darkturquoise}{gears_5}\\
& + \textcolor{blue}{b_4}\textcolor{darkturquoise}{(disp\times gears_4)}\\
& + \textcolor{blue}{b_5}\textcolor{darkturquoise}{(disp\times gears_5)}\\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

- Interactions occur when predictors are _multiplied_
- In this case, $\textcolor{darkturquoise}{disp}$ is multiplied by $\textcolor{darkturquoise}{gears_4}$ and $\textcolor{darkturquoise}{gears_5}$
- $\textcolor{darkturquoise}{gears}$ now changes the intercept and the slope of the relationship between $mpg$ and $\textcolor{darkturquoise}{disp}$
::::

:::

## How do I get R to fit this model?

\tiny
```{r, echo=TRUE} 
#mpg depends on disp interacted (*) with gears
mod2 <- lm(mpg ~ disp*factor(gear), data = mtcars) 
summary(mod2)
```
\small
Beware of fitting too many interactions, or else the \emph{Bilbo effect} occurs!

## Dummy variables
\tiny
```{r, echo=TRUE, eval=FALSE} 
mod2Matrix <- model.matrix(mod2) #Get model matrix (columns used to predict mpg)
colnames(mod2Matrix) <- gsub('(factor\\(|\\))','',colnames(mod2Matrix)) #Shrink column headers
head(mod2Matrix,20) #Show first 20 rows of model matrix
```

## Third challenge

- Make some plots of your model results using `ggeffects`

- If you're feeling bold, try adding an __interaction__ term to your model

- Interaction syntax:

  - `lm (y ~ X1 * X2)` and `lm (y ~ X1 + X2 + X1:X)` do the same thing

- If you have more than 2 terms, you can specify certain interactions like this:

  - `lm (y ~ X1 * X2 * X3)`: Full model (everything interacts)
  - `lm (y ~ X1 + X2 + X3 + X2:X3)`: interaction only between `X2` and `X3`

# Part 3: Models behaving badly

```{r setup3, include=FALSE}
set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Generate data that violate lm assumptions:
n <- 100
x <- runif(n,-10,10)
corMat <- matrix(c(1,0.95,0.95,1),ncol=2) #Correlation matrix (r=0.95)
x2 <- (cbind(x,runif(n,-10,10)) %*% chol(corMat))[,2] #Makes correlated predictor x2 via Cholesky matrix of corMat
x3 <- x+1e5
yhat <- 1 - 0.2*x #Expected value
y0 <- yhat + rnorm(n,0,2) #OK
y1 <- yhat + 0.1*x^2 + rnorm(n,0,2) #Polynomial function
y2 <- rpois(n,exp(yhat))  #Poisson process
y3 <- rbinom(n,1,invLogit(yhat))  #Binomial process
y4 <- (1 - 2e-4*(x3))+rnorm(n,0,1e-3)
d1 <- data.frame(x,x2,x3,yhat,y0,y1,y2,y3,y4)
d1$s.y4 <- scale(y4)
d1$s.x3 <- scale(x3)
# rm(n,x,x2,yhat,y0,y1,y2,y3)

```


## Motivation 

Are my models behaving themselves?

- Residual checks

- Transformations

- Collinearity

- How much stuff should I put into my model?
  

## Assumptions of linear regression

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
meanDisp <- mean(mtcars$disp) #Mean displacement
y_meanDisp <- coef(modp1) %*% c(1,meanDisp) #Predicted y at mean displacement
b0Lab <- paste('b[0] == ',round(coef(modp1)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp1)[2],3)) 

p1 + theme(axis.title.x.bottom=element_text(colour='darkturquoise'),axis.text.x.bottom=element_text(colour='darkturquoise'))
```

::::

:::: column

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}


There are 3 main assumptions to this model:

\pause

1. The relationship between $\textcolor{darkturquoise}{disp}$ and $mpg$ is __linear__
2. $mpg$ (the data) is __Normally distributed__ around $\textcolor{orange}{\hat{mpg}}$ (the line)
3. $\textcolor{red}{\sigma}$ is the __same everywhere__

\pause

This is pretty easy to see if you only have 1 variable, but...

::::

::: 

## What if I have many variables?

::: columns

:::: column

```{r, fig.height=3, fig.width=3}
p3
```

::::

:::: column

- Difficult to see if the assumptions are met
- In general, we use __residual plots__ or __simulation__ to assess whether model assumptions are met

::::

:::

## Solution: residual checks

Some common ways of checking the assumptions: __residual plots__

```{r, echo=FALSE, eval=FALSE} 
mod1 <- lm(mpg~disp*factor(gear),data=mtcars) #Fits model
plot(mod1, which=c(1,2)) #1st and 2nd residual plots - others don't add much
```

```{r, echo=FALSE, out.width='100%', fig.asp=0.4} 
mod1 <- lm(mpg~disp*factor(gear),data=mtcars) #Fits model
par(mfrow=c(1,2),mar=c(3,3,1,1)+1) #Splits plot into 2
plot(mod1, which=c(1,2)) #1st and 2nd residual plots - others don't add much
```


- Points in Plot 1 should show _no pattern_ (shotgun blast)
- Points in Plot 2 should be _roughly_ on top of the 1:1 line

## Problem 1: Non-linear relationship

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(x,y1))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

Model: `lm(y1~x,data=d1)`

- $y1$ clearly follows a U-shaped relationship, not a linear one

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m1 <-lm(y1~x,data=d1) 
plot(m1, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: transform predictors

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2} 
ggplot(d1,aes(x,y1))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~poly(x,2),col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

Model: `lm(y1~poly(x,2),data=d1)`

- _log_ and _square-root_ transformations are common
- Can also use _additive_ (wiggly) models

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=2.5} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y1~poly(x,2),data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```
- Warning: Polynomials can do weird things; consider whether this is biologically reasonable!

::::

:::

## Problem 2a: Non-normal response

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(x,y2))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+geom_hline(yintercept=0,linetype='dashed')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

Model: `lm(y2~x,data=d1)`

- $y2$ is count data (integers $\geq{0}$). _Very_ common in ecological data.

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y2~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: transform data to meet assumptions

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(x,log(y2+1)))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

Model: `lm(log(y2+1)~x,data=d1)`

- Square-root transformations are also common

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(log(y2+1)~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## Problem 2b: Non-normal response

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(x,y3))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

Model: `lm(y3~x,data=d1)`

- $y3$ is binomial data (success/failure). _Very_ common in ecological data.

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y3~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: use a Generalized Linear Model (GLM)

```{r, echo=FALSE, fig.width=5, fig.height=2.5} 
ggplot(d1,aes(x,y3))+
  geom_point()+
  geom_smooth(method='glm',se=TRUE,formula=y~x,method.args=list(family='binomial'),col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

- This is a topic for another lecture. Hold tight!


## Problem: variables are on different scales

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(x3,y4))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```
\small

Model: `lm(y4~x3,data=d1)`

- $y4$ is tiny, while $x3$ is huge
- OK for now, but can cause problems when fitting other models

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y4~x3,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::


## Solution: scale data/predictors before fitting

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
ggplot(d1,aes(scale(x3),scale(y4)))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+
  theme(plot.margin = unit(c(0.1,0.1,0,0.1),'cm'))
```

\small

```{r echo=TRUE, eval=FALSE}
#Subtracts mean, divides by SD
d1$s.y4 <- scale(y4)
d1$s.x3 <- scale(x3) 
lm(s.y4~s.x3,data=d1) #Refit
```


::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=2.5} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(s.y4~s.x3,data=d1)
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0,cex=0.75)
```

- Residuals are the same as before
- Coefficients are now related to _scaled_ data and predictor

::::

:::


## But wait... there's more (assumptions)!

::: columns

:::: column

One more assumption:

4. If you have 2+ predictors in your model, the predictors are not related to each other

- Say we have 2 predictors, $x$ and $x2$:
  `lm(y0~x+x2,data=d1)`
- Model fits, and residuals look OK, but there's trouble ahead!

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y0~x+x2,data=d1) 
plot(m2, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
```

::::

:::

## Uh oh! Collinearity!
::: columns

:::: column


```{r, echo=TRUE, fig.width=4,fig.height=3} 
#Function to print correlation (r) value
corText <- function(x,y){
  text(0.5,0.5,round(cor(x,y),3))} 
#Pairplot of y0, x, and x2
pairs(d1[,c('y0','x','x2')],
      lower.panel=corText)
```


::::

:::: column

\pause

- $x$ and $x2$ mean basically the same thing!
- Also revealed using variance-inflation factors (VIFs):

\pause

\normalsize

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(car) 
#VIF scores:
# 1 = no problem
# 1-5 = some problems
# 5+ = big problems!
vif(m2) 
```

\pause

Causes:

- Structural: one term is a function of the other
- Data: other underlying (possibly unmeasured) relationships

::::

:::

## Is collinearity really that bad?

- Increases SE of each term, so model may "miss" important terms

- Gets worse with increasing correlation, or if many terms are correlated

\pause

::: columns

:::: column

```{r, echo=TRUE} 
#Correct model
m1 <- lm(y0~x,data=d1)
```

\tiny

```{r, echo=FALSE,eval=TRUE} 
kable(summary(m1)$coef[,-3]) %>% row_spec(2,color='red')
```

::::

:::: column 

```{r, echo=TRUE} 
#Incorrect model
m2 <- lm(y0~x+x2,data=d1)
```

\tiny

```{r, echo=FALSE,eval=TRUE} 
kable(summary(m2)$coef[,-3]) %>% row_spec(2,color='red')
```

::::

:::


## How do we fix this? Depends on your goals:

::: columns

:::: column

1. I _only_ care about predicting things
- Use dimensional reduction (e.g. PCA) and re-run lm model
- Use a machine learning model (e.g. Random Forest)

2. I care about what's causing things
- Design experiment to separate cause and effect
- Think about __what__ is causing __what__. Graphical models are helpful for this
  - Not all variables have to be included!

::::

:::: column

\pause

```{r echo=FALSE, fig.height=2.5,fig.width=3,message=FALSE}  
library(ggdag)

#Coordinates for nodes
nodeCoords <- data.frame(name=LETTERS[1:4], labs=LETTERS[1:4], 
                         x=c(0,1,1,2),y=c(0,1,-1,0))

#Define model using dagify
tidy_ggdag <- tidy_dagitty(dagify(
  D ~ C,
  D ~ B,
  B ~ A,
  coords= list(x = setNames(nodeCoords$x,nodeCoords$name),
               y = setNames(nodeCoords$y,nodeCoords$name)),
  labels=setNames(nodeCoords$labs,nodeCoords$name)
))

tidy_ggdag$data %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+ 
  geom_dag_edges(arrow_directed=arrow(angle=20,type='open'))+
  geom_dag_point(shape=0) +
  geom_dag_text(aes(label=label),size=6,col='black')+
  lims(x=c(-0.25,2.25),y=c(-1.25,1.25))+
  theme_dag_blank()
```

- Simple graphical model, where the effect of A on D is _mediated_ by B.
- "Correct" `lm` model of D:
  - `lm(D ~ B + C)`

::::

:::


## Fourth challenge

- Let's say you're an ecologist studying foraging. You're interested in predicting `bats` (bat calls per night), and there are 6 variables that you measured that might _somehow_ relate to bat foraging,
- Formulate a graphical model that seems reasonable
  - Draw it out on paper/in PowerPoint using flow diagrams 
- Fit an `lm` model of `bats` using your graphical model, check the assumptions, and update as necessary

\large
```{r, echo=FALSE,eval=TRUE}
dat <- read.csv('./batDat.csv')
head(dat)
```


## Here's the answer

```{r echo=FALSE, fig.width=7,fig.height=2.5}  

#Coordinates for nodes
nodeCoords2 <- data.frame(name=c('humidity','temp','bugs','bats','light','clouds'),
                         x=c(0,1,0.5,1.5,2,2.5),y=c(0.5,1,0,0,1,0))

#Define model using dagify
tidy_ggdag2 <- tidy_dagitty(dagify(
  humidity ~ temp,
  bugs ~ temp,
  bats ~ bugs,
  bats ~ temp,
  bats ~ light,
  light ~ clouds,
  coords= list(x = setNames(nodeCoords2$x,nodeCoords2$name),
               y = setNames(nodeCoords2$y,nodeCoords2$name)),
  labels=setNames(nodeCoords2$name,nodeCoords2$name)
))

tidy_ggdag2$data %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+ 
  geom_dag_edges(arrow_directed=arrow(angle=20,type='open'))+
  # geom_dag_point(shape=NA) +
  geom_dag_text(aes(label=label),size=6,col='black')+
  lims(x=c(-0.25,3.25),y=c(-0.25,1.25))+
  theme_dag_blank()

```

This is the __true__ process that generated the data. Model for `bats` should look like:

`lm(logbats~poly(temp,2)+light+bugs,data=mutate(dat,logbats=log(bats+0.1)))`

## Model results 

```{r, echo=FALSE,warning=FALSE,message=FALSE,fig.height=3,fig.width=7}

dat <- dat %>% mutate(logBats=log(bats+1)) #Pre-transform

mod1 <- lm(logBats~poly(temp,2)+log(light)+bugs,data=dat)

pltThm <- theme_classic()+theme(plot.title=element_blank())

q1 <- ggpredict(mod1,terms='temp') %>% plot(residuals=TRUE)+pltThm
q2 <- ggpredict(mod1,terms='light') %>% plot(residuals=TRUE)+pltThm
q3 <- ggpredict(mod1,terms='bugs') %>% plot(residuals=TRUE)+pltThm

ggarrange(q1,q2,q3,nrow=1,ncol=3)

```

## To do this week: draw out your hypotheses

::: columns

:::: column

Create a graphical model of your own data

1. List all of the things you measured as _boxes_
2. Think about how things might go together
3. Draw relationships between ovals using _arrows_
4. Fit some starter models of your data, check whether they met the assumptions
5. Make some simple plots!

::::

:::: column

```{r, echo=FALSE, fig.width=3,fig.height=3}
#Define model using dagify
tidy_ggdag <- tidy_dagitty(dagify(
  D ~ C,
  D ~ B,
  B ~ A,
  A ~~ C,
  coords= list(x = setNames(nodeCoords$x,nodeCoords$name),
               y = setNames(nodeCoords$y,nodeCoords$name)),
  labels=setNames(nodeCoords$labs,nodeCoords$name)
))

tidy_ggdag$data %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+ 
  geom_dag_edges(aes(edge_linetype=ifelse(direction=='<->','dashed','solid')),
    arrow_directed=arrow(angle=20,type='open'),
                 arrow_bidirected=arrow(angle=20,ends = "both", type ="open"),
                 curvature=0
                 )+
  geom_dag_point(shape=0) +
  geom_dag_text(aes(label=label),size=6,col='black')+
  lims(x=c(-0.25,2.25),y=c(-1.25,1.25))+
  theme_dag_blank()



```

::::

:::
