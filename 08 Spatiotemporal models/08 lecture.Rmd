---
title: "Spatiotemporal models"
subtitle: '"Space is the place" - Sun Ra'
author: "Samuel Robinson, Ph.D."
date: "Oct 27, 2023"
output: 
  beamer_presentation:
    incremental: false
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
    fig_caption: false
urlcolor: blue
df_print: kable
classoption: aspectratio=169   
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
  - \let\oldShaded\Shaded %Change fontsize of R code chunks
  - \let\endoldShaded\endShaded
  - \renewenvironment{Shaded}{\scriptsize\oldShaded}{\endoldShaded}
  - \let\oldverbatim\verbatim %Change fontsize of code chunk output
  - \let\endoldverbatim\endverbatim
  - \renewenvironment{verbatim}{\tiny\oldverbatim}{\endoldverbatim}
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)
library(sf)
library(gstat)
library(mgcv)
library(glmmTMB)
library(ggeffects)

knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, 
                      warning=TRUE, cache = TRUE, fig.height=5,fig.width=5)

#Example dataframe for date/time conversions
dExamp <- data.frame(x=c(5,10),
                     d1=c('2010-05-06','2021-11-14'),
                     d2=c('2010-06-13','2022-10-14'))

set.seed(12)

#Function to make temporally correlated noise (AR1, see https://en.wikipedia.org/wiki/Autoregressive_model)
makeAR1 <- function(phi=0.5,x0=0,N=100,sigma=1){
  x <- rep(0,N)
  sigma <- rnorm(N,0,sigma)
  x[1] <- x0
  for(i in 2:N) x[i] <- x[i-1]*phi + sigma[i]
  return(x)
}

#Generate data (correlated intercepts/slopes)
n <- 200
ngroups <- 20
x <- runif(n,-10,10) #Single fixed effect predictor
g <- sample(letters[1:ngroups],n,TRUE) #Groups
intercept <- 1
slopeX <- 0.5
sigmaR <- 3 #Residual sigma 
sigmaG <- 5 #Group intercept sigma
sigmaG_slope <- abs(slopeX*2) #Slope sigma (half slope value)

#Correlated intercepts and slopes, using Choleski matrices
raneffs <- matrix(rnorm(ngroups*2,0,1),ncol=2) #Uncorrelated unit normals
slopeCor <- 0.7 #Intercept-slope correlation
corMat <- matrix(c(1,slopeCor,slopeCor,1),ncol=2) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski transform of corMat
raneffs <- raneffs %*% cholCorMat #Induces correlation in slopes
raneffs <- raneffs * matrix(rep(c(sigmaG,sigmaG_slope),each=ngroups),ncol=2,
                            dimnames=list(letters[1:ngroups],c('Int','Slope'))) #Changes SD for each column
raneff_int <- model.matrix(~g-1) %*% raneffs[,1] #Intercept vector
raneff_slope <- x * model.matrix(~g-1) %*% raneffs[,2]  #Slope vector

yhat <- intercept + slopeX*x + raneff_int + raneff_slope  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data (normal)
y2 <- rnbinom(n,mu=exp(yhat/5),sigmaR) #Data (NB)
dat <- data.frame(y,y2,x,site=g) #Assemble into data frame

#Generate spatially correlated random intercepts

#Squared-exponential distance function
covFun <- function(sigma,rho,d) (sigma^2)*exp((-rho^2)*(d^2)) 

lat <- runif(ngroups,-10,10) #"Latitude"
lon <- runif(ngroups,-10,10) #"Longitude"
distMat <- as.matrix(dist(cbind(lat,lon),diag=TRUE,upper=TRUE)) #Matrix of distances
covMat <- covFun(sigmaG,0.1,distMat) #Covariance matrix
corMat <- cov2cor(covMat) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski matrix
raneffs <- rnorm(ngroups,0,1) %*% cholCorMat #Induce correlation
raneff_int <- model.matrix(~g-1) %*% raneffs[1,] #Intercept vector
yhat <- intercept + slopeX*x + raneff_int  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data (normal)
y2 <- rnbinom(n,mu=exp(yhat/5),sigmaR) #Data (NB)
lat <- model.matrix(~g-1) %*% lat
lon <- model.matrix(~g-1) %*% lon
dat2 <- data.frame(y,y2,x,site=g,lat,lon) #Assemble into data frame

# Generate temporally correlated random intercepts
time <- runif(ngroups,-10,10)
# time <- seq(-20,20,length.out=20)
distMat <- as.matrix(dist(time,diag=TRUE,upper=TRUE)) #Matrix of distances
diag(distMat) <- 1e-5
covMat <- covFun(sigmaG,0.1,distMat) #Covariance matrix
corMat <- cov2cor(covMat) #Correlation matrix
cholCorMat <- chol(corMat,pivot = TRUE) #Choleski matrix
raneffs <- rnorm(ngroups,0,1) %*% cholCorMat #Induce correlation
raneff_int <- model.matrix(~g-1) %*% raneffs[1,] #Intercept vector
yhat <- intercept + slopeX*x + raneff_int  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data (normal)
# y2 <- rnbinom(n,mu=exp(yhat/5),sigmaR) #Data (NB)
# lat <- model.matrix(~g-1) %*% lat
# lon <- model.matrix(~g-1) %*% lon
time <- model.matrix(~g-1) %*% time
dat3 <- data.frame(y,x,site=g,time) #Assemble into data frame

```

## Outline

::: columns

:::: column

- Spatial and temporal data
  - Some basic GIS (`sf`)
- How to think about space and time
  - Plotting
  - Variograms
  - "Continuous" random effects
  - Kernels and
- Some common modeling approaches
  - GLS (covariance)
  - Basis functions (GAMs)

::::

:::: column

Spaaaaace

::::

:::

## Some common problems

- My data were sampled over time or space. I'm not really interested in time or space _per se_, so can I just ignore them and run my models?
- I am actually interested in how something changes over time or space. Can I just use day or location (lat/lon) as another term in my model?
- My supervisor told me to look for something called autocorrelation, and it sounds scary

## A common approach: random effects

"Can I just use day or site as a random effect?"

- Short answer: "Yes"
- Long answer: You might be able to do better, because of the __1st Law of Geography__:

_"...everything is related to everything else, but near things are more related than distant things."_ Waldo Tobler

- If you have spatial or temporal information, this can help R to estimate random effects more accurately
  - Can improve prediction accuracy (smaller p-values)
  - Can give you hints about the underlying causal mechanisms

# Part 1: Time and Space in R

## How R deals with time

::: columns

:::: column

- Dealing with time in R is somewhat annoying, but not complicated
- Common methods: `as.Date` (days), `as.POSIXlt` (date + time)
- Both require a date/time format: see `?strptime` for examples
- You can transform to specific formats (e.g. day of year) using `format`
- `difftime` is useful for getting differences in time points

::::

:::: column

```{r}
dExamp
```

 

```{r, echo=TRUE}
#Convert data to Date format
dateForm <- '%Y-%m-%d'
dExamp %>% 
  mutate(across(c(d1,d2),
            ~as.Date(.x,format=dateForm))) %>% 
  #Get day of year
  mutate(doy=format(d1,format='%j')) %>% 
  #Get difference in time between d2 and d1
  mutate(dChange=difftime(d2,d1,units='days'))
```


::::

:::

## Two main types of spatial data

::: columns

:::: column

__Vector__ data: points, lines, and polygons

```{r}
# par(mar=c(0,0,0,0))
exampPoly1 <- data.frame(x=c(0.13,0.10,0.21,0.44,0.62,0.66,0.58,0.37,0.13),
                         y=c(0.47,0.62,0.84,0.91,0.86,0.68,0.55,0.47,0.47))
exampPoly2 <- data.frame(x=c(0.4,0.53,0.92,0.71,0.4),
                         y=c(0.09,0.31,0.33,0.08,0.09))
plot(0,0,xlim=c(0,1),ylim=c(0,1),type='n',xlab='x',ylab='y')
points(exampPoly1); lines(exampPoly1)
points(exampPoly2)
```
R packages: `sf`, `sp`, `gstat`, `spdep`

::::

:::: column

__Raster__ data: cells

```{r}
image(volcano,xlab='x',ylab='y')
```

R packages: `stars`, `terra`
 
::::

:::

## R as a GIS

- A __Geographic Information System__ (GIS) is a system for organizing, analyzing, and displaying spatial information
- Common platforms and tools: ArcGIS, QGIS, PostGIS, Python
- A number of R packages are specifically written for dealing with GIS data, usually specific to raster or vector formats
- Ecologists mostly deal with vector data (site locations, boundary polygons) but raster data is sometimes used (NDVI, land cover classes)
- I'll show you a couple practical tips for using the `sf` package (see [here](https://r-spatial.github.io/sf/index.html) also), but there are [many other packages](https://r-spatial.org/book/) out there

\vspace{0.5cm}

If you're dealing with large amounts of spatial data _I would encourage you to take a formal GIS course_, as there is a LOT to learn!

## Common tasks: making maps

::: columns

:::: column

- Vector data are often encoded as _shapefiles_ (set of several files)
- Point data can also be read in as _csv_ files, which need to be turned into an `sf` object
- `sf` objects can be displayed in `ggplot` using `geom_sf`. Common aesthetics (colour, size) can be mapped onto the plot
  - Objects are layered on the map in order of coding
- Be careful: shapefiles can be very large, which can easily crash R!

::::

:::: column

```{r abMap, warning=FALSE}
#Reads AB boundary shapefile
abBound <- read_sf('./shapefiles/AB_only.shp') %>% 
  st_transform(4326)

#Reads city csv
csvPath <- './shapefiles/abCities.csv'
abCities <- read.csv(csvPath) %>% 
  #Converts to sf
  st_as_sf(coords = c('lon','lat'),crs=4326) 
#NOTE: crs 4326 is common lat/lon format

#Make map
(p1 <- ggplot()+ 
  #Add boundary
  geom_sf(data=abBound)+ 
  #Add cities
  geom_sf(data=abCities,aes(size=pop),col='red')+
  #Add labels
  geom_sf_text(data=abCities,aes(label=name),
  size=3,nudge_y=0.25)+
  labs(x=NULL,y=NULL,size="Population"))
```

::::

:::

## Common tasks: making maps (cont.)

::: columns

:::: column

```{r abMap, echo=TRUE,eval=FALSE, warning=FALSE}
```

::::

:::: column

```{r abMap, warning=FALSE}
```

::::

:::


## Common tasks: reprojection

::: columns

:::: column

```{r abMapProj, warning=FALSE}
abBound_utm <- abBound %>% st_transform(3401)
abCities_utm <- abCities %>% st_transform(3401)

p2 <- ggplot()+ 
  geom_sf(data=abBound_utm)+ 
  geom_sf(data=abCities_utm,aes(size=pop),
          col='red',show.legend = FALSE)+
    geom_sf_text(data=abCities_utm,aes(label=name),
               size=3,nudge_y=25000)+
  labs(x=NULL,y=NULL,size="Population")
ggarrange(p1+guides(size='none'),
          p2,ncol=2,labels = c('WGS 84','UTM-10'))
```


::::

:::: column

- The world is not flat: all maps have to "bend" the data somehow. This is called the map __projection__
- Some map projections preserve _area_, others preserve _distance_. Degrees are not all the same distance apart!
- Usually we're interested in absolute distance between locations, so _Mercator_ (UTM) is a good choice, but be careful which UTM zone you choose!
- `sf` uses `crs` codes: __4326__ is for lat/lon (WGS 84), 3401 is an Alberta-specific UTM projection
- [Many](epsg.io) others are available


::::

:::

## First challenge

::: columns

:::: column

- Make this map of bird counts from the ABMI dataset
- `medDetects` is the median detection rate at each site over several years


::::

:::: column

```{r abmiMap}
abBound <- read_sf('./shapefiles/AB_only.shp') %>% 
  st_transform(3401)
birdDat <- read.csv('./shapefiles/birdDat.csv') %>% 
  st_as_sf(coords = c('lon','lat'),crs=4326) %>% 
  st_transform(st_crs(abBound))
abCities <- read.csv('./shapefiles/abCities.csv') %>% 
  st_as_sf(coords = c('lon','lat'),crs=4326) %>% 
  st_transform(st_crs(abBound))

ggplot()+
  geom_sf(data=birdDat,aes(col=log(medDetects)))+
  geom_sf(data=abBound,fill=NA,col='black')+
  geom_sf(data=abCities,col='red',size=1)+
  facet_wrap(~Common.Name)+
  labs(col='Detection')+
  theme(axis.text = element_text(size=8),
        legend.position=c(0.85,0.25))
  
```

::::

:::

## First challenge results

::: columns

:::: {.column width='60%'}

```{r abmiMap, eval=FALSE,echo=TRUE}
```

::::

:::: {.column width='40%'}

```{r abmiMap}
```

::::

:::


# Part 2: Spatiotemporal modeling

## Let's start with an example

::: columns

:::: column

- Say we're fitting a simple linear regression on a dataset collected across space

```{r}
dat2_small <- dat2 %>% select(y,x,site,lat,lon) %>% 
  group_by(site) %>% slice(1) %>% ungroup()
head(dat2_small)
```

```{r,echo=FALSE}
m1 <- lm(y~x,data=dat2_small)
```

```{r, warning=FALSE, message=FALSE}
plot(ggpredict(m1,terms='x'),residuals = TRUE)+
  labs(title='Model: lm(y ~ x)')
```

::::

:::: column

Things look mostly OK, right?

```{r resPlot}
par(mfrow=c(2,1),mar=c(0,0,0,0))
plot(m1, which=c(1,2),caption='',cex=0.75,cex.axis=1e-5,tck=0)
par(mfrow=c(2,1),mar=c(5.1,4.1,4.1,2.1))
```

::::

:::

## Spatial residual plot

::: columns

:::: column

- Residuals are spatially _non-independent_!
- _Variograms_ are a common tool to examine how variance changes with distance
- Uncorrelated spatial data will have a _flat_ variogram (no change in semivariance with distance)

```{r vgExample}
dat2_small %>% mutate(res=residuals(m1)) %>%
  st_as_sf(coords=c('x','y')) %>% 
  as_Spatial() %>% #convert to an sp object
  variogram(res~1,data=.) %>% #calculate covariance at each distance
  ggplot(aes(x=dist,y=gamma))+geom_line()+geom_point()+labs(x='Distance',y='Semivariance')

```

::::

:::: column

```{r spResPlot}
dat2_small %>% 
  mutate(res=residuals(m1)) %>% 
  ggplot()+
  geom_point(aes(x=x,y=y,size=abs(res),col=res))+
  labs(title='Residuals across space')+
  guides(size='none')+
  theme(legend.position = c(0.8,0.3))
```


::::

:::



## Temporal or Spatial Data

- Correlation is often present in temporal data or spatial data; causes may be unknown or "uninteresting"
- Usually we are interested in accounting for these patterns, in order to better estimate the "interesting" patterns on top of them
- Last week we talked about _cross_-correlation (i.e. correlation between columns of data); this week we're talking about _auto_-correlation (i.e. correlation between individual data points in a single column)

::: columns

:::: column

```{r, fig.height=3, fig.width=4}
# data.frame(year=as.numeric(time(lynx)),lynxNum=as.numeric(lynx)) %>%
#   ggplot(aes(x=year,y=lynxNum))+geom_line()+
#   labs(x='Year',y='Lynx Numbers')
plot(sunspots,ylab='Sunspots per year')
```

::::

:::: column

```{r, fig.height=3, fig.width=4}
# ggplot(seals,aes(x=long,y=lat,fill=delta_long))+
#   geom_raster()+
#   scale_fill_continuous(type='viridis')+
#   labs(x='Longitude',y='Latitude')+
#   theme(legend.position = c(0.2,0.7))
image(volcano)
```

::::

:::

## Covariance

- Normal distributions\footnotemark don't just have a single $\sigma$, but a matrix of values
- If our data _y_ are _independent_, then it looks like this:

\begin{equation*}
y \sim Normal(\textcolor{orange}{M},\textcolor{red}{\Sigma})
\end{equation*}

\begin{equation*}
\textcolor{orange}{M} = [\mu_1, \mu_2, \mu_3]
\end{equation*}

\begin{equation*}
\textcolor{red}{\Sigma} = \begin{bmatrix}
\textcolor{red}{\sigma}^2 & 0 & 0 \\
0 & \textcolor{red}{\sigma}^2 & 0 \\
0 & 0 & \textcolor{red}{\sigma}^2
\end{bmatrix}
\end{equation*}

- Zeros mean "$\mu_1$, $\mu_2$, \& $\mu_3$ aren't related to each other"
- Diagonal elements = _variance_, off-diagonal = _covariance_

\footnotetext{Multivariate Normal}

## Covariance and Correlation

In real life, things may not be independent from each other. For example:

- $\textcolor{red}{\sigma}$ = 2 (variance = $\textcolor{red}{\sigma}^2$ = 4)
- $\mu_1$ and $\mu_2$ are strongly correlated (r=0.7), but $\mu_3$ is not related to anything (r=0). Shown here as a _correlation matrix_ ($\textcolor{red}{R}$):

\begin{equation*}
\textcolor{red}{R} = \begin{bmatrix}
1 & 0.7 & 0 \\
0.7 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\end{equation*}

- When multiplied by the variance, this becomes the _covariance matrix_ ($\textcolor{red}{\Sigma}$)

\begin{equation*}
\textcolor{red}{\Sigma} = \begin{bmatrix}
\textcolor{red}{\sigma}^2\times1 & \textcolor{red}{\sigma}^2\times0.7 & \textcolor{red}{\sigma}^2\times0 \\
\textcolor{red}{\sigma}^2\times0.7 & \textcolor{red}{\sigma}^2\times1 & \textcolor{red}{\sigma}^2\times0 \\
\textcolor{red}{\sigma}^2\times0 & \textcolor{red}{\sigma}^2\times0 & \textcolor{red}{\sigma}^2\times1
\end{bmatrix} = \begin{bmatrix}
4 & 2.8 & 0 \\
2.8 & 4 & 0 \\
0 & 0 & 4
\end{bmatrix}
\end{equation*}


## Gaussian Process Modelling

::: columns

:::: column

- We can model covariance between things as a function of _distance_, either in time or space
- Squared-exponential is fairly common\footnotemark:

\begin{equation*}
\begin{split}
\textcolor{red}{\Sigma} = & covariance \\
\textcolor{red}{\Sigma} = & variance \times correlation \\
\textcolor{red}{\Sigma} = & \textcolor{red}{\sigma}^2 \times e^{-\textcolor{green}{\rho}^2 Dist^2}\\
\end{split}
\end{equation*}

- Instead of finding a single $\textcolor{red}{\sigma}$ value, R now looks for $\textcolor{red}{\sigma}$ (maximum covariance) and $\textcolor{green}{\rho}$ (decay with distance)

::::

:::: column

```{r, fig.height=5,fig.width=4}
expand.grid(dist=seq(0,5,0.1),rho=c(0.5,1,2),sigma=c(1,2)) %>%
  mutate(cVal=(sigma^2)*exp((-rho^2)*(dist^2))) %>%
  mutate(category=paste0('sigma=',sigma,', rho=',rho)) %>%
  mutate(rho=factor(rho)) %>%
  ggplot(aes(x=dist,y=cVal,group=category,col=rho))+geom_line(size=1)+
  labs(x='Distance (km)',y='Covariance')+scale_colour_manual(values=c('red','purple','blue'))+
  theme(legend.position = c(0.8,0.8))
```

::::

:::

\footnotetext{Also common: AR-1 (temporal processes), Mat\'{e}rn (spatial processes)}

## Spatial random effects

::: columns

:::: column

- Say that we collected data at 16 sites, and we're interested in the effect of _y_ on _x_
- Let's first fit a model with a random intercept for site

```{r, echo=TRUE,size='tiny'}
#Same syntax as lmer models:
lmm2 <- glmmTMB(y~x+(1|site),data=dat2)
```

::::

:::: column

- If we plot the intercepts for each site, we see that they are clustered:

```{r,echo=FALSE,fig.height=3,fig.width=4,size='tiny'}
dat2 %>% select(site,lat,lon) %>% distinct() %>% arrange(site) %>% mutate(int=lmm2$sdr$par.random) %>%
  ggplot(aes(lon,lat,col=int))+geom_point(aes(size=abs(int)))+
  labs(title='Random intercepts (1|Site)',x='Longitude',y='Latitude',col='Intercept')+
  scale_colour_gradient2(low='blue',mid='purple',high='red')+
    guides(size='none')
```

::::

:::


## Spatial random effects (cont.)

::: columns

:::: column

- Re-fit model with a spatial (exponential) random effect

```{r, echo=TRUE,size='tiny'}
#Coordinates
dat2$coords <- numFactor(dat2$lon,dat2$lat)

#Group factor (only 1 here)
dat2$group <- factor(rep(1,nrow(dat2)))

#Fit model with spatial random effect
lmm3 <- glmmTMB(y~x+exp(coords+0|group),data=dat2)
```

::::

:::: column

- Clustering effect modeled as a spatial random effect

```{r, echo=FALSE, fig.width=4,fig.height=3,size='tiny'}
#Spatial random effect field
spRanEff <- expand.grid(lon=-10:10,lat=-10:10) %>%
  mutate(coords=numFactor(lon,lat),group=factor(rep(1,nrow(.))),x=0) %>%
  mutate(pred=predict(object=lmm3,newdata=.,type='response',allow.new.levels = TRUE))

#Predictions at each data location
ranIntPred <- dat2 %>% select(site:group) %>% distinct() %>% mutate(x=0) %>% 
  mutate(int=predict(object=lmm3,newdata=.,type='response')-lmm3$fit$par[1]) #Random intercepts

ggplot(spRanEff,aes(lon,lat))+
  geom_raster(aes(fill=pred))+
  # geom_point(data=ranIntPred,aes(lon,lat,fill=NULL),col='red')+
  geom_point(data=ranIntPred,aes(size=abs(int),col=int))+
  labs(x='Longitude',y='Latitude',
       title='Spatial random effect',fill='Intercept')+
  scale_fill_viridis_c(option='cividis',aesthetics = 'fill')+
  scale_colour_gradient2(low='blue',mid='purple',high='red',aesthetics = 'col')+
  guides(size='none',col='none')

```


::::

:::

## Challenge

## Problem: hard for large datasets

## Solution: basis function

## A challenger approaches

- Ho ho ho! Merry Christmas! In order to maximize the number of presents that you get from Santa Claus, you've decided to apply an analytic approach, and have collected data across Alberta on _number of Christmas presents received_
- You've also collected data on things that might influence Saint Nick's generosity (_naughtiness_, _presence of milk and cookies_, _chimney width_)
- Fit a GLMM to the present data, one using spatial random intercepts, and one using "regular" random intercepts
- Which type of snack should you leave out for Santa? Which area might you consider moving to??

## Two-column slide

::: columns

:::: column

::::

:::: column

::::

:::

