---
title: "Spatiotemporal models"
subtitle: '"Space is the place" - Sun Ra'
author: "Samuel Robinson, Ph.D."
date: "Oct 27, 2023"
output: 
  beamer_presentation:
    incremental: false
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
    fig_caption: false
urlcolor: blue
df_print: kable
classoption: aspectratio=169   
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  - \useinnertheme{circles}
  - \let\oldShaded\Shaded %Change fontsize of R code chunks
  - \let\endoldShaded\endShaded
  - \renewenvironment{Shaded}{\scriptsize\oldShaded}{\endoldShaded}
  - \let\oldverbatim\verbatim %Change fontsize of code chunk output
  - \let\endoldverbatim\endverbatim
  - \renewenvironment{verbatim}{\tiny\oldverbatim}{\endoldverbatim}
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_classic())
library(sf)
library(mgcv)
library(glmmTMB)
library(ggeffects)

knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message=TRUE, 
                      warning=TRUE, cache = FALSE, fig.height=5,fig.width=5)

#Example dataframe for date/time conversions
dExamp <- data.frame(x=c(5,10),
                     d1=c('2010-05-06','2021-11-14'),
                     d2=c('2010-06-13','2022-10-14'))

set.seed(12)

#Generate data (correlated intercepts/slopes)
n <- 200
ngroups <- 20
x <- runif(n,-10,10) #Single fixed effect predictor
g <- sample(letters[1:ngroups],n,TRUE) #Groups
intercept <- 1
slopeX <- 0.5
sigmaR <- 3 #Residual sigma 
sigmaG <- 5 #Group intercept sigma
sigmaG_slope <- abs(slopeX*2) #Slope sigma (half slope value)

#Correlated intercepts and slopes, using Choleski matrices
raneffs <- matrix(rnorm(ngroups*2,0,1),ncol=2) #Uncorrelated unit normals
slopeCor <- 0.7 #Intercept-slope correlation
corMat <- matrix(c(1,slopeCor,slopeCor,1),ncol=2) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski transform of corMat
raneffs <- raneffs %*% cholCorMat #Induces correlation in slopes
raneffs <- raneffs * matrix(rep(c(sigmaG,sigmaG_slope),each=ngroups),ncol=2,
                            dimnames=list(letters[1:ngroups],c('Int','Slope'))) #Changes SD for each column
raneff_int <- model.matrix(~g-1) %*% raneffs[,1] #Intercept vector
raneff_slope <- x * model.matrix(~g-1) %*% raneffs[,2]  #Slope vector

yhat <- intercept + slopeX*x + raneff_int + raneff_slope  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data (normal)
y2 <- rnbinom(n,mu=exp(yhat/5),sigmaR) #Data (NB)
dat <- data.frame(y,y2,x,site=g) #Assemble into data frame

#Generate spatially correlated random intercepts

#Squared-exponential distance function
covFun <- function(sigma,rho,d) (sigma^2)*exp((-rho^2)*(d^2)) 

lat <- runif(ngroups,-10,10) #"Latitude"
lon <- runif(ngroups,-10,10) #"Longitude"
distMat <- as.matrix(dist(cbind(lat,lon),diag=TRUE,upper=TRUE)) #Matrix of distances
covMat <- covFun(sigmaG,0.1,distMat) #Covariance matrix
corMat <- cov2cor(covMat) #Correlation matrix
cholCorMat <- chol(corMat) #Choleski matrix
raneffs <- rnorm(ngroups,0,1) %*% cholCorMat #Induce correlation
raneff_int <- model.matrix(~g-1) %*% raneffs[1,] #Intercept vector
yhat <- intercept + slopeX*x + raneff_int  #Expected value
y <- rnorm(n,yhat,sigmaR) #Data (normal)
y2 <- rnbinom(n,mu=exp(yhat/5),sigmaR) #Data (NB)
lat <- model.matrix(~g-1) %*% lat
lon <- model.matrix(~g-1) %*% lon
dat2 <- data.frame(y,y2,x,site=g,lat,lon) #Assemble into data frame

```

## Outline

::: columns

:::: column

- Spatial and temporal data
  - Some basic GIS (`sf`)
- How to think about space and time
  - Plotting
  - Variograms
  - "Continuous" random effects
  - Kernels and
- Some common modeling approaches
  - GLS (covariance)
  - Basis functions (GAMs)

::::

:::: column

Spaaaaace

::::

:::

## Some common problems

- My data were sampled over time or space. I'm not really interested in time or space _per se_, so can I just ignore them and run my models?
- I am actually interested in how something changes over time or space. Can I just use day or location (lat/lon) as another term in my model?
- My supervisor told me to look for something called autocorrelation, and it sounds scary

## A common approach: random effects

"Can I just use day or site as a random effect?"

- Short answer: "Yes"
- Long answer: You might be able to do better, because of the __1st Law of Geography__:

_"...everything is related to everything else, but near things are more related than distant things."_ Waldo Tobler

- If you have spatial or temporal information, this can help R to estimate random effects more accurately
  - Can improve prediction accuracy (smaller p-values)
  - Can give you hints about the underlying causal mechanisms

# Part 1: Time and Space in R

## How R deals with time

::: columns

:::: column

- Dealing with time in R is somewhat annoying, but not complicated
- Common methods: `as.Date` (days), `as.POSIXlt` (date + time)
- Both require a date/time format: see `?strptime` for examples
- You can transform to specific formats (e.g. day of year) using `format`
- `difftime` is useful for getting differences in time points

::::

:::: column

```{r}
dExamp
```
```{r, echo=TRUE}
dExamp %>% 
  mutate(across(c(d1,d2),~as.Date(.x,format='%Y-%m-%d'))) %>% 
  mutate(doy=format(d1,format='%j')) %>% 
  mutate(dChange=difftime(d2,d1,units='days'))
```


::::

:::

## Two main types of spatial data

::: columns

:::: column

__Vector__ data: points, lines, and polygons

```{r}
exampPoly1 <- data.frame(x=c(0.13,0.10,0.21,0.44,0.62,0.66,0.58,0.37,0.13),
                         y=c(0.47,0.62,0.84,0.91,0.86,0.68,0.55,0.47,0.47))
exampPoly2 <- data.frame(x=c(0.4,0.53,0.92,0.71,0.4),
                         y=c(0.09,0.31,0.33,0.08,0.09))
plot(0,0,xlim=c(0,1),ylim=c(0,1),type='n',xlab='x',ylab='y')
points(exampPoly1); lines(exampPoly1)
points(exampPoly2); lines(exampPoly2)
```
Common R packages: `sf`, `sp`, `gstat`, `spdep`

::::

:::: column

__Raster__ data: cells

```{r}
image(volcano,xlab='x',ylab='y')
```

Common R packages: `stars`, `terra`
 
::::

:::

## R as a GIS

- A __Geographic Information System__ (GIS) is a system for organizing, analyzing, and displaying spatial information
- Common platforms and tools: ArcGIS, QGIS, PostGIS, Python
- A number of R packages are specifically written for dealing with GIS data, usually specific to raster or vector formats
- Ecologists mostly deal with vector data (site locations, boundary polygons) but raster data is sometimes used (NDVI, land cover classes)
- I'll show you a couple practical tips for using the `sf` package (see [here](https://r-spatial.github.io/sf/index.html) also), but there are [many other packages](https://r-spatial.org/book/) out there
\vspace{0.5cm}
If you're dealing with large amounts of spatial data _I would encourage you to take a formal GIS course_, as there is a LOT to learn!

## Common tasks: making maps

::: columns

:::: column

- Vector data are often encoded as _shapefiles_ (set of several files)
- Point data can also be read in as _csv_ files, which need to be turned into an `sf` object
- `sf` objects can be displayed in `ggplot` using `geom_sf`. Common aesthetics (colour, size) can be mapped onto the plot
  - Objects are layered on the map in order of coding
- Be careful: shapefiles can be very large, which can easily crash R!

::::

:::: column

```{r abMap}
#Reads AB boundary shapefile
abBound <- read_sf('./shapefiles/AB_only.shp')

#Reads city csv
csvPath <- './shapefiles/abCities.csv'
abCities <- read.csv() %>% 
  #Converts to sf
  st_as_sf(coords = c('lon','lat'),crs=4326) 
#NOTE: crs 4326 is common lat/lon format

#Make map
ggplot()+ 
  #Add boundary
  geom_sf(data=abBound)+ 
  #Add cities
  geom_sf(data=abCities,aes(size=pop),col='red')+
  #Add labels
  geom_sf_text(data=abCities,aes(label=name),
               size=3,nudge_y=25000)+
  labs(x=NULL,y=NULL,size="Population")
```

::::

:::

## Common tasks: making maps (cont.)

::: columns

:::: column

```{r abMap, echo=TRUE, eval=FALSE}
```

::::

:::: column

```{r abMap}
```

::::

:::


## Common tasks: reprojection





# Part 2: Spatiotemporal modeling

## Temporal or Spatial Data

- Last week we talked about _cross_-correlation (i.e. correlation between columns of data); this week we're mostly talking about _auto_-correlation (i.e. correlation between individual data points in a single column)
- Correlation is often present in temporal data or spatial data; causes may be unknown or "uninteresting"
- Usually we are interested in accounting for these patterns, in order to better estimate the "interesting" patterns on top of them

::: columns

:::: column

```{r, fig.height=4, fig.width=4}
# data.frame(year=as.numeric(time(lynx)),lynxNum=as.numeric(lynx)) %>%
#   ggplot(aes(x=year,y=lynxNum))+geom_line()+
#   labs(x='Year',y='Lynx Numbers')
plot(sunspots,ylab='Sunspots per year')
```

::::

:::: column

```{r, fig.height=4, fig.width=4}
# ggplot(seals,aes(x=long,y=lat,fill=delta_long))+
#   geom_raster()+
#   scale_fill_continuous(type='viridis')+
#   labs(x='Longitude',y='Latitude')+
#   theme(legend.position = c(0.2,0.7))
image(volcano)
```

::::

:::

## Covariance

- Normal distributions\footnotemark don't just have a single $\sigma$, but a matrix of values
- If our data _y_ are _independent_, then it looks like this:

\begin{equation*}
y \sim Normal(\textcolor{orange}{M},\textcolor{red}{\Sigma})
\end{equation*}

\begin{equation*}
\textcolor{orange}{M} = [\mu_1, \mu_2, \mu_3]
\end{equation*}

\begin{equation*}
\textcolor{red}{\Sigma} = \begin{bmatrix}
\textcolor{red}{\sigma}^2 & 0 & 0 \\
0 & \textcolor{red}{\sigma}^2 & 0 \\
0 & 0 & \textcolor{red}{\sigma}^2
\end{bmatrix}
\end{equation*}

- Zeros mean "$\mu_1$, $\mu_2$, \& $\mu_3$ aren't related to each other"
- Diagonal elements = _variance_, off-diagonal = _covariance_

\footnotetext{Multivariate Normal}

## Covariance and Correlation

In real life, things may not be independent from each other. For example:

- $\textcolor{red}{\sigma}$ = 2 (variance = $\textcolor{red}{\sigma}^2$ = 4)
- $\mu_1$ and $\mu_2$ are strongly correlated (r=0.7), but $\mu_3$ is not related to anything (r=0). Shown here as a _correlation matrix_ ($\textcolor{red}{R}$):

\begin{equation*}
\textcolor{red}{R} = \begin{bmatrix}
1 & 0.7 & 0 \\
0.7 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\end{equation*}

- When multiplied by the variance, this becomes the _covariance matrix_ ($\textcolor{red}{\Sigma}$)

\begin{equation*}
\textcolor{red}{\Sigma} = \begin{bmatrix}
\textcolor{red}{\sigma}^2\times1 & \textcolor{red}{\sigma}^2\times0.7 & \textcolor{red}{\sigma}^2\times0 \\
\textcolor{red}{\sigma}^2\times0.7 & \textcolor{red}{\sigma}^2\times1 & \textcolor{red}{\sigma}^2\times0 \\
\textcolor{red}{\sigma}^2\times0 & \textcolor{red}{\sigma}^2\times0 & \textcolor{red}{\sigma}^2\times1
\end{bmatrix} = \begin{bmatrix}
4 & 2.8 & 0 \\
2.8 & 4 & 0 \\
0 & 0 & 4
\end{bmatrix}
\end{equation*}


## Gaussian Process Modelling

::: columns

:::: column

- We can model covariance between things as a function of _distance_, either in time or space
- Squared-exponential is fairly common\footnotemark:

\begin{equation*}
\begin{split}
\textcolor{red}{\Sigma} = & covariance \\
\textcolor{red}{\Sigma} = & variance \times correlation \\
\textcolor{red}{\Sigma} = & \textcolor{red}{\sigma}^2 \times e^{-\textcolor{green}{\rho}^2 Dist^2}\\
\end{split}
\end{equation*}

- Instead of finding a single $\textcolor{red}{\sigma}$ value, R now looks for $\textcolor{red}{\sigma}$ (maximum covariance) and $\textcolor{green}{\rho}$ (decay with distance)

::::

:::: column

```{r, fig.height=5,fig.width=4}
expand.grid(dist=seq(0,5,0.1),rho=c(0.5,1,2),sigma=c(1,2)) %>%
  mutate(cVal=(sigma^2)*exp((-rho^2)*(dist^2))) %>%
  mutate(category=paste0('sigma=',sigma,', rho=',rho)) %>%
  mutate(rho=factor(rho)) %>%
  ggplot(aes(x=dist,y=cVal,group=category,col=rho))+geom_line(size=1)+
  labs(x='Distance (km)',y='Covariance')+scale_colour_manual(values=c('red','purple','blue'))+
  theme(legend.position = c(0.8,0.8))
```

::::

:::

\footnotetext{Also common: AR-1 (temporal processes), Mat\'{e}rn (spatial processes)}

## Spatial random effects

::: columns

:::: column

- Say that we collected data at 16 sites, and we're interested in the effect of _y_ on _x_
- Let's first fit a model with a random intercept for site

```{r, echo=TRUE,size='tiny'}
#Same syntax as lmer models:
lmm2 <- glmmTMB(y~x+(1|site),data=dat2)
```

::::

:::: column

- If we plot the intercepts for each site, we see that they are clustered:

```{r,echo=FALSE,fig.height=3,fig.width=4,size='tiny'}
dat2 %>% select(site,lat,lon) %>% distinct() %>% arrange(site) %>% mutate(int=lmm2$sdr$par.random) %>%
  ggplot(aes(lon,lat,col=int))+geom_point(aes(size=abs(int)))+
  labs(title='Random intercepts (1|Site)',x='Longitude',y='Latitude',col='Intercept')+
  scale_colour_gradient2(low='blue',mid='purple',high='red')+
    guides(size='none')
```

::::

:::


## Spatial random effects (cont.)

::: columns

:::: column

- Re-fit model with a spatial (exponential) random effect

```{r, echo=TRUE,size='tiny'}
#Coordinates
dat2$coords <- numFactor(dat2$lon,dat2$lat)

#Group factor (only 1 here)
dat2$group <- factor(rep(1,nrow(dat2)))

#Fit model with spatial random effect
lmm3 <- glmmTMB(y~x+exp(coords+0|group),data=dat2)
```

::::

:::: column

- Clustering effect modeled as a spatial random effect

```{r, echo=FALSE, fig.width=4,fig.height=3,size='tiny'}
#Spatial random effect field
spRanEff <- expand.grid(lon=-10:10,lat=-10:10) %>%
  mutate(coords=numFactor(lon,lat),group=factor(rep(1,nrow(.))),x=0) %>%
  mutate(pred=predict(object=lmm3,newdata=.,type='response',allow.new.levels = TRUE))

#Predictions at each data location
ranIntPred <- dat2 %>% select(site:group) %>% distinct() %>% mutate(x=0) %>% 
  mutate(int=predict(object=lmm3,newdata=.,type='response')-lmm3$fit$par[1]) #Random intercepts

ggplot(spRanEff,aes(lon,lat))+
  geom_raster(aes(fill=pred))+
  # geom_point(data=ranIntPred,aes(lon,lat,fill=NULL),col='red')+
  geom_point(data=ranIntPred,aes(size=abs(int),col=int))+
  labs(x='Longitude',y='Latitude',
       title='Spatial random effect',fill='Intercept')+
  scale_fill_viridis_c(option='cividis',aesthetics = 'fill')+
  scale_colour_gradient2(low='blue',mid='purple',high='red',aesthetics = 'col')+
  guides(size='none',col='none')

```


::::

:::

## Challenge

## Problem: hard for large datasets

## Solution: basis function

## A challenger approaches

- Ho ho ho! Merry Christmas! In order to maximize the number of presents that you get from Santa Claus, you've decided to apply an analytic approach, and have collected data across Alberta on _number of Christmas presents received_
- You've also collected data on things that might influence Saint Nick's generosity (_naughtiness_, _presence of milk and cookies_, _chimney width_)
- Fit a GLMM to the present data, one using spatial random intercepts, and one using "regular" random intercepts
- Which type of snack should you leave out for Santa? Which area might you consider moving to??

## Two-column slide

::: columns

:::: column

::::

:::: column

::::

:::

