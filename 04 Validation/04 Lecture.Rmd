---
title: "Linear models 3"
subtitle: "Models behaving badly"
author: "Samuel Robinson, Ph.D."
date: "October 23, 2020"
output: 
  beamer_presentation:
    theme: "default"
    colortheme: "lily"
    highlight: "tango"
df_print: kable
header-includes: 
  \definecolor{darkturquoise}{rgb}{0.0, 0.81, 0.82}
  \useinnertheme{circles}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_classic())
library(ggpubr)
library(knitr)
library(kableExtra)

set.seed(123)

#Functions
logit <- function(x) log(x/(1-x))
invLogit <- function(x) exp(x)/(1+exp(x))

#Plots of mtcars data
#Continuous
p1 <- ggplot(arrange(mtcars,disp),aes(x=disp,y=mpg))+geom_point()+geom_smooth(formula=y~x,method='lm',se=FALSE,col='orange')

#Interaction
p3 <- ggplot(mtcars,aes(x=disp,y=mpg,col=factor(gear)))+geom_point()+labs(col='gears')+
  geom_smooth(formula=y~x,method='lm',se=FALSE)+
  scale_colour_manual(values=c('blue','purple','red'))+theme(legend.justification=c(1,1), legend.position=c(1,1))

#Fit models for later use
modp1 <- lm(mpg~disp,data=arrange(mtcars,disp))
modp3 <- lm(mpg~disp*factor(gear),data=mtcars)

#Generate data that violate lm assumptions:
n <- 100
x <- runif(n,-10,10)
corMat <- matrix(c(1,0.95,0.95,1),ncol=2) #Correlation matrix (r=0.95)
x2 <- (cbind(x,runif(n,-10,10)) %*% chol(corMat))[,2] #Makes correlated predictor x2 via Cholesky matrix of corMat
x3 <- x+1e5
yhat <- 1 - 0.2*x #Expected value
y0 <- yhat + rnorm(n,0,2) #OK
y1 <- yhat + 0.1*x^2 + rnorm(n,0,2) #Polynomial function
y2 <- rpois(n,exp(yhat))  #Poisson process
y3 <- rbinom(n,1,invLogit(yhat))  #Binomial process
y4 <- (1 - 2e-4*(x3))+rnorm(n,0,1e-3)
d1 <- data.frame(x,x2,x3,yhat,y0,y1,y2,y3,y4)
d1$s.y4 <- scale(y4)
d1$s.x3 <- scale(x3)
# rm(n,x,x2,yhat,y0,y1,y2,y3)

```


## Motivation 

1. Are my model results reliable?
- Residual checks
- Transformations
- Collinearity
2. How do I tell if terms are important or not?
- Drop-1 ANOVA
- Wald t-tests
- How much stuff should I put into my model?
  
# Are my model results reliable?

## Assumptions of linear regression
::: columns

:::: column

```{r, fig.height=3, fig.width=3}
meanDisp <- mean(mtcars$disp) #Mean displacement
y_meanDisp <- coef(modp1) %*% c(1,meanDisp) #Predicted y at mean displacement
b0Lab <- paste('b[0] == ',round(coef(modp1)[1],3))
b1Lab <- paste('b[1] == ',round(coef(modp1)[2],3)) 

p1 + theme(axis.title.x.bottom=element_text(colour='darkturquoise'),axis.text.x.bottom=element_text(colour='darkturquoise'))
```

\begin{equation*} 
\begin{split}
\textcolor{orange}{\hat{mpg}} & = \textcolor{blue}{b_0} + \textcolor{blue}{b_1}\textcolor{darkturquoise}{disp} \\
mpg & \sim Normal(\textcolor{orange}{\hat{mpg}},\textcolor{red}{\sigma})
\end{split}
\end{equation*}

::::

:::: column

There are 3 main assumptions to this model:

1. The relationship between $\textcolor{darkturquoise}{disp}$ and $mpg$ is linear
2. $mpg$ (the data) is Normally distributed around $\textcolor{orange}{\hat{mpg}}$ (the line)
3. $\textcolor{red}{\sigma}$ is the same everywhere

This is pretty easy to see if you only have 1 variable, but...

::::

::: 

## What if I have many variables?

```{r, fig.height=3, fig.width=5}
p3
```

Difficult to see if the assumptions are met

## Solution: residual checks

Some common ways of checking the assumptions: __residual plots__

\tiny

```{r, echo=TRUE, out.width='100%', fig.asp=0.5} 
mod1 <- lm(mpg~disp*factor(gear),data=mtcars)
par(mfrow=c(1,2),mar=c(3,3,1,1)+1)
plot(mod1, which=c(1,2))
```

\normalsize

1. Points in Plot 1 should show _no pattern_ (shotgun blast)
2. Points in Plot 2 should be _roughly_ on top of the 1:1 line

## Problem 1: Non-linear relationship

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,y1))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')
```

```{r echo=TRUE, eval=FALSE}
lm(y1~x,data=d1)
```

$y1$ clearly follows a hump-shaped relationship

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m1 <-lm(y1~x,data=d1) 
plot(m1, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: transform predictors

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,y1))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~poly(x,2),col='orange')
```

```{r echo=TRUE, eval=FALSE}
lm(y1~poly(x,2),data=d1)
```

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y1~poly(x,2),data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

Warning: Polynomials can do weird things, especially at the edges of the distribution. Consider whether this is biologically reasonable!

## Problem 2a: Non-normal response

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,y2))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')+geom_hline(yintercept=0,linetype='dashed')
```

```{r echo=TRUE, eval=FALSE}
lm(y2~x,data=d1)
```

$y2$ is count data (integers $\geq{0}$). _Very_ common in ecological data.

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y2~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: transform data to meet assumptions

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,log(y2+1)))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')
```

```{r echo=TRUE, eval=FALSE}
lm(log(y2+1)~x,data=d1)
```
Square-root transformations are also common

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(log(y2+1)~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Problem 2b: Non-normal response

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x,y3))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')
```

```{r echo=TRUE, eval=FALSE}
lm(y3~x,data=d1)
```

$y3$ is binomial data (success/failure, 0 or 1). _Very_ common in ecological data.

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y3~x,data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: use a Generalized Linear Model (GLM)

```{r, echo=FALSE, fig.width=5, fig.height=3} 
ggplot(d1,aes(x,y3))+geom_point()+geom_smooth(method='glm',se=TRUE,formula=y~x,method.args=list(family='binomial'),col='orange')
```

This is a topic for another lecture. Hold tight!


## Problem: variables are on different scales

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(x3,y4))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')
```
\small

```{r echo=TRUE, eval=FALSE}
lm(y4~x3,data=d1)
```

- $y4$ is tiny, while $x3$ is huge
- This is fine for now, but can cause problems when fitting more complicated models
  - e.g. GLMs, GLMMs

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y4~x3,data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Solution: scale data/predictors before fitting

::: columns

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=3} 
ggplot(d1,aes(scale(x3),scale(y4)))+geom_point()+geom_smooth(method='lm',se=TRUE,formula=y~x,col='orange')
```

\small

```{r echo=TRUE, eval=FALSE}
#Subtracts mean, divides by SD
d1$s.y4 <- scale(y4)
d1$s.x3 <- scale(x3) 
lm(s.y4~s.x3,data=d1) #Refit
```


::::

:::: column

```{r, echo=FALSE, fig.width=2, fig.height=2.5} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(s.y4~s.x3,data=d1)
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

\small

- Residuals are the same as before
- Coefficients are now related to _scaled_ data and predictor

::::

:::


## But wait... there's more (assumptions)!

::: columns

:::: column

- Additional assumption for models with many predictors:

4. If you have 2+ predictors in your model, the predictors are not related to each other

- Say we have 2 predictors, $x$ and $x2$:

```{r echo=TRUE, eval=FALSE}
lm(y0~x+x2,data=d1)
```

- Model fits, and residuals look OK, but there's trouble ahead!

::::

:::: column

```{r, echo=FALSE, fig.width=3, fig.height=4} 
par(mfrow=c(2,1),mar=c(0,0,0,0))
m2 <-lm(y0~x+x2,data=d1) 
plot(m2, which=c(1,2),caption='',cex.axis=1e-5,tck=0)
```

::::

:::

## Here comes trouble!
::: columns

:::: column

\tiny
```{r, echo=TRUE, fig.width=4,fig.height=3} 
#Function to print correlation (r) value
corText <- function(x,y){
  text(0.5,0.5,round(cor(x,y),3))
} 

#Pairplot of y0, x, and x2
pairs(d1[,c('y0','x','x2')],lower.panel=corText)
```

::::

:::: column

- $x$ and $x2$ mean basically the same thing!
- Also revealed using variance-inflation factors (VIFs):

\small

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(car)
#VIF scores:
# 1 = no problem
# 1-5 = some problems
# 5+ = big problems!
vif(m2) 
```

::::

:::

## Is this problem really that bad?

::: columns

:::: column

\tiny
```{r, echo=TRUE} 
#Correct model
m1 <- lm(y0~x,data=d1)
```

```{r, echo=FALSE,eval=TRUE} 
kable(summary(m1)$coef[,-3]) %>% row_spec(2,color='red')
```

::::

:::: column 

\tiny
```{r, echo=TRUE} 
#Incorrect model
m2 <- lm(y0~x+x2,data=d1)
```

```{r, echo=FALSE,eval=TRUE} 
kable(summary(m2)$coef[,-3]) %>% row_spec(2,color='red')
```

::::

:::

- Increases SE of each term, so model may "miss" important terms
- Gets worse with increasing correlation, or if many terms are correlated!

## How do we fix this? Depends on your goals:

::: columns

:::: column

1. I care about predicting things
- Use dimensional reduction (e.g. PCA) and re-run model

2. I care about what's causing things
- Design experiment to separate cause and effect
- Think about what is causing what. _Graphical models_ are helpful for this
  - Not all variables have to be included!

::::

:::: column


```{r echo=FALSE, out.width='80%',fig.cap='A simple graphical model'}  
  include_graphics('./dag.png',dpi=NA)
```

::::

:::

